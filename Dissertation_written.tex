\documentclass{article} 
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx} 

\usepackage{fullpage}
\usepackage[parfill]{parskip}

\title{Dissertation}
\author{Cesar Esparza}

\newpage
\begin{document} 
\maketitle
Introduction
\\\\ In nature every living creature has specific characteristics that determine their role in their environments. Each living creature from microscopic entities to a blue whale or the tallest tree, posses patterns that allow them to live, some longer than other. We perceive many similarities among living, born at certain point, grow, reproduce and die, that is the path for all. But as similarities exist also differences, living entities in cold environments have different obvious characteristics to others in a hotter environment. But yet despite of those different physical differences that exist I dare to say the most interesting is behaviour. It defines role every existing element has, hunter and prey is the first idea that comes to my mind when talking about roles. But we can go deeper in the nature of these to an we can see that the behaviour is different, but a prey can also be a hunter. That is the essence of behaviour how particular it is to each type of organism although the same goal is always the intention, survival.  As humans we might be one of the most complex creatures in our environment. For some basic needs like eating to survive is not something to be concerned about. And in my mind this is one of the reasons our behaviours have changed, one can say they have evolved, I like to call it adapting. The basic rules of survival  have changed for humans, a big percentage of the humans do not hunt to eat anymore,  some do farming and act as providers for the others. And the key word for me is rules, rules dictate who we are and how we behave  around others. It is bold to say that we all play a game, life, and the rules are not only dictated by our natural environment but also by the social environment. Something I find interesting is asking the question, why do we play for?
\\\\ In this work the intention is not to follow the existential line of previous paragraph, but to give a general framework of a particular branch of game theory and building a program in python to solve simple problems presented in the normal form. The program built in Python will use a simplified genetic algorithm. And the approach of the model is called Agent based modelling, which will be introduced in another chapter. Basic relevant knowledge about evolutionary game theory will be presented. And the Python program will be used to solve some well known normal form games (games is strategic form) like prisoners' dilemma, rock-paper-scissors, matching pennies, and some others. We will also use the program to simulate a an experiment made by the professor in political science Robert Axelrod in 1980, the objective was to see how cooperation evolved in an environment where selfish behaviours exist when interacting through time. The experiment consisted in a tournament where several well-known game theorists participated by providing a strategy that was set to interact with strategies from others, and then determine which strategy was more successful.  
 
\newpage
\maketitle
\begin{abstract}
The literature review can be found in the following section. Some history of Game theory will be mentioned, Nash equilibrium is briefly mentioned but still important since the concept of equilibrium is important to discuss in game theory, simple concept about normal form games, history of evolutionary game theory and an explanation of what evolutionary game theory is and the introduction of the concept of equilibrium in evolutionary game theory which is known as evolutionary stable strategies(ESS), and a brief explanation of the similarities with Nash equilibrium.
% This will need a lot of work - note that you need various abstracts, summaries
% etc\dots


\end{abstract}


\newpage
\section{Game Theory}\label{first_section}
Game theory or theory of games is a wildly known theory, which studies the interaction of decisions. This interaction is given to the interdependence between the participants in an environment. Resulting from this there can be mainly two types of interaction cooperative or competitive (Watson 2013)$\cite{watson2013strategy}$. The whole concept of game theory has a very wide application in many different sciences like economics, companies in a competing market, setting import and exporting tariffs, agreements in wages between employers and employees, auctions, to name some(Gibbons 1992)$\cite{gibbons1992primer}$, political sciences Nolan McCarthy mentions examples such as raising more money than other candidates when running for a political position and what to do when the opposition is considered “weak” or “strong”, accepting new policies if the outcomes are not certain,  an example when a jury has to sentence a defendant among others(McCarty et al 2007)$\cite{mccarty2007political}$, in psychology when studying the behavior of individuals with respect of others, although application of strict assumptions of game theory such as rationality been assumed in the same level for all participants still is a controversial topic for the applying game theory in many areas of social psychology (Rapoport 1999)$\cite{rapoport1999game}$ (Colman 2003)$\cite{colman2003cooperation}$ , biology which will be mentioned further in relation with evolutionary game theory, and other disciplines. Studies that started exploring the potential solution of some games can be traced back to 1713  when James Waldegrave in a letter communicated a mixed solution for a two person game to his colleague Pierre Remond de Montmort (Hykšová 2004)$\cite{hykvsova2004several}$ . Since Waldegrave many others did studies that now we relate to game theory (Watson 2013)$\cite{watson2013strategy}$.  We will start from the french mathematician Emile Borel who in a note in his work ``La th\'{e}orie du jeu et les \'{e}quations, int\'{e}grales \`{a} noyau sym\'{e}trique gauche'' (Borel 1921)$\cite{borel:1921}$ mentioned:
\\\\ ``The problems of probability and analysis suggest themselves concerning the art of war, or economic or financial speculations, are not without analogy with problems concerning games, though they generally have a higher degree of complication''.
\\\\ In 1924 Borel on his note ``On games that involve chance and the skill of the Players'' (Borel 1953)$\cite{borel1953games}$ he mentions that ``the study of games that involve at once chance and the skill of the players appears to me similarly able to furnish an opportunity for mathematical research, the applications of which might far surpass the limits of the restricted domain to which this first study is limited. /such research might be extended to very many questions in which psychological unknowns figure along with algebraic unknowns.'' Then Borel continues saying that the only author who had studied problems with this focus was Joseph Bertrand in his ``Calcul des Probalites'' in 1889, distinguishing between mathematical and psychological aspects in an example given of a game of baccarat, but goes on stating why Bertrand study was incomplete (Borel 1953)$\cite{borel1953games}$.
\\\\ Game Theory was further researched and formally presented by the Hungarian mathematician John von Neumann in 1928 with his work “Theory of Parlor Games”, stating in the introduction that ``… any event – given external conditions and the participants situation (provided the latter are acting of their own free will) – may be regarded as a game of strategy if one looks at the effect it has on the participants.” (von Neumann 1928)$\cite{von1928theory}$. And in 1944 John von Neumann and Oskar Morgestern published “Theory of Games and Economic Behavior” in which they established theory of games of strategy as an instrument to study problems in the economic behavior (Neumann, Morgestern 1944)$\cite{von2007theory}$. 
Despite of the great contribution of von Neumann and Morgestern, game theory became widely used after the Doctoral dissertation from John F. Nash was published in 1950. The some of the main arguments from Nash’s work are that he gives the possibility of analyzing games with more n-players, he introduces the concept of non-cooperative games having at least one equilibrium point and gives the idea of a “dynamical” approach to study cooperative games (Nash 1950)$\cite{nash1951non}$. The last point I mentioned refers to how a cooperative game can be reduced to non-cooperative form, because the idea of a cooperative game established by von Neumann and Morgenstern was that cooperation was given under the assumption described in Nash’s words “players can communicate and form coalitions which will be enforced by and umpire.”(Nash 1950)$\cite{nash1951non}$ , but he proves this condition is not the only one that would define a cooperative game. The work from Nash opened the possibility of a broader application of the essential concept of game theory, many people have extended the fundamental concepts adopted from Nash’s work and a lot of studies for the development of game theory have been made.

\subsection{Normal form games}\label{second_section}
(Watson 2013)$\cite{watson2013strategy}$  mentions that there are several mathematical ways to describe games. In game theory there are two most common ways to represent a game called extensive form and normal form. The extensive form which represent in a form of a game tree the different actions that can be taken by each player. Starts with an initial node from which it branches out representing the possible choices and then after each branch another node is placed where a second player has other choices that branch out, until reaching the end of the tree where the payoff for each sequence(following the branches) for the path are represented. But for the purpose of this work, the normal form representation of a game will be used and the basic information about it is the following.
The normal form games, also known as strategic form of games, usually have the following elements:
\begin{itemize}
\item $\textbf{Players}$ A finite set of $\textit{N}$ players.
\item $\textbf{Strategies:}$ A set S$_i$ for each player $\textit{i}$ $\in$ $\textit{N}$. Which as we have discussed represents the action a player chooses.
\item $\textbf{Payoffs:}$ Payoff functions represented for each player represented by $\textit{u$_i$: S$_1$ x S$_2$ x}$ ... $\textit{ x S$_N$}$ $\rightarrow$ $\mathbb{R}$ .Which represent the payoff each player obtain after interacting with each other.
\end{itemize}

For the purpose of this work, the number of players will be $\textit{N}$ = 2 taking the following assumptions from (Knight ????) we will represent the game with a $\textbf{ bi-matrix}$. And we will assume that  $\textbf{S$_1$ = \{s$_i$ $|$ 1  $\geq$ i $\geq$ n\}}$ and $\textbf{S$_2$ = \{s$_j$ $|$ 1  $\geq$ j $\geq$ n\}}$ this will be the $\textbf{ bi-matrix}$ for this game :

\begin{center}
Player 2

Player 1
\begin{tabular}{|l|c|c|c|c|}
\hline
& s$_1$ & s$_2$ & ... & s$_j$\\ 
\hline
r$_1$ & u$_1$(r$_1$,s$_1$), u$_2$(r$_1$,s$_1$) & u$_1$(r$_1$,s$_2$), u$_2$(r$_1$,s$_2$) & ... &  u$_1$(r$_1$,s$_j$), u$_2$(r$_1$,s$_j$\\
\hline
r$_2$ & u$_1$(r$_2$,s$_1$), u$_2$(r$_2$,s$_1$) & u$_1$(r$_2$,s$_2$), u$_2$(r$_2$,s$_2$) & ... &  u$_1$(r$_2$,s$_j$), u$_2$(r$_2$,s$_j$\\
\hline
. & . & . & ... & .\\
. & . & . & ... & .\\
. & . & . & ... & .\\
\hline
r$_i$ & u$_1$(r$_i$,s$_j$), u$_2$(r$_i$,s$_j$) & u$_1$(r$_i$,s$_j$), u$_2$(r$_i$,s$_j$) & ... &  u$_1$(r$_i$,s$_j$), u$_2$(r$_i$,s$_j$)\\
\hline
\end{tabular}



$\textbf{Table 1: Normal form game bi-matrix}$
\end{center}

We can see that each intersection of row and column have two utility functions. The first utility function represents the payoff for the row player and the second utility function represents the payoff for the column player. Utilities can be of type ordinal, which is used only to rank the alternatives from better to worse, or cardinal which indicates that the value assigned is meaningful and represents the satisfaction of the player (McCarty et al 2007)$\cite{mccarty2007political}$.


\subsection{Nash Equilibrium}\label{third_section}
John Nash writes in his dissertation ``... an equilibrium point is an n-tuple s so that each player's mixed strategy maximizes his payoff if the strategies of the others are held fixed. Thus each player's strategy is optimal against those of the others.'' (Nash 1950)$\cite{nash1951non}$.  After this he describes under what circumstances a mixed strategy behaves as a pure strategy. Nowadays the definition of Nash's equilibrium point is known as a Nash equilibrium and we will commonly find the phrase "no regrets" when describing the Nash equilibrium and this is because paraphrasing Nash, no player in the game could have had a better payoff given the action chosen by her opponent. In (Osborne 2004)$\cite{osborne2004introduction}$ a definition for Nash equilibrium of strategic games with ordinal preferences, where `` a$^*$ represents an action(strategy) profile in a strategic game with ordinal preferences, and it is a Nash equilibrium if for every player $\textit{i}$ and every action $\textit{a$_i$}$ of player $\textit{i}$, $\textit{a$^*$}$ is at least as good according to player $\textit{i's}$ preferences as the action profile $\textit{(a$_i$, a$^*$$_{-i}$)}$  in which player $\textit{i}$ chooses $\textit{a$_i$}$ while every other player $\textit{j}$ chooses $\textit{a$^*$$_j$}$ along with the following inequality. Where $\textit{u$_i$}$ represents player $\textit{i}$'s preferences.'':
\begin{equation}
u_i(a^*)  {\geq}  u_i(a_i, a^* _{-i}))
\end{equation}

\subsection{Evolutionary game theory}
Evolutionary game theory has its roots in evolutionary biology. Even if this overview is not intended to be focused in biology some remarks from it are worth mentioning. From the 6th edition of ‘Origin of species’ Charles Darwin outlines that in nature there exist many struggles for existence. Some examples are the struggles of a species in the nature, between species and within species. Darwin stresses that the most severe struggle might be within species if they become into competition (Darwin 1872)$\cite{darwin1872origin}$. In origin of species with the idea of all possible interactions of the `organic’ beings mentions that there may be infinite varied diversities of structure for each being under the changing conditions of life, and continues saying that through the course of generations, there can occur variations that can give perhaps a slightly advantage to some beings giving them a higher chance of survival and procreation. The preservation of favourable characteristics and destruction of what he called injurious, he called it natural selection or survival of the fittest (Darwin 1872)$\cite{darwin1872origin}$. I go this far because later on we can think of the concept of fitness, as the ‘utility’ a player has in evolutionary game theory which in general terms can be called the ‘fittest’.

Fisher might have been the first to apply game theory to evolution in his study on sex ratios in 1930 (Pallen 2009)$\cite{pallen2009rough}$ although at this time the formal definition of game theory had not been presented yet. Later in 1961 Lewontin discussed species playing against nature, and said that species should adopt the ``maximin'' strategy if nature presented worse-case scenarios (Maynard 1974, Sigmund 2004)$\cite{sigmund2004maynard}$. In 1967 in an the article ‘Extraordinary sex ratios’ wrote in ‘Science’ William D. Hamilton uses game theory to model local competition and frequency-dependent fitness values (Sigmund 2004)$\cite{sigmund2004maynard}$. Perhaps the most important contribution in evolutionary game theory was the one made by Professor John Maynard and Dr. George Price. John Maynard’s attention was first caught by the article ''Antlers, Intraspecific Combat, and Altruism',' written by George Price in 1968 for `Nature’ about ritualized behavior in animal contests (Maynard 1976)$\cite{smith1976evolution}$ unpublished for being too long. Then in 1973 John Maynard and George Price published a joint paper ‘On the logic of animal conflict’ in which the mathematical concept of an evolutionary stable strategy is established, and it applies concepts of game theory to the study of conflicts between animals (Sigmund 2004)$\cite{sigmund2004maynard}$.  The idea Maynard and Price presented was that concepts from game theory could be used to characterize eventually stable endpoints in the evolutionary process, with the concept of evolutionary stable strategy (McNamara 2010)$\cite{mcnamara2010evolutionary}$. It can be said that the concept of evolutionary game theory was born with the ideas of John Maynard and George Price.

Nowadays evolution by natural selection can be thought of as a game, where some behavioural patterns (often referred to as phenotypes the equivalent to strategies when related to traditional game theory) from animals are more successful than others (Carmichael 2005)$\cite{carmichael2005guide}$. Now I can relate evolutionary game theory with traditional game theory in the some essential concepts. Animals in the biological concept are equivalent to the players (agents) that participate in a game; the environment in which animals interact is comparable to the set of rules that regulate interaction in the traditional form; as mentioned before the heritable phenotypes of animals can be thought of as the strategies that players use in the traditional form; a tricky concept to relate to evolutionary game theory is the one of payoffs (utility) in traditional game theory  for this I will refer to how Maynard and Price define it in ‘The logic of animal conflict’ as the contribution the contest has made to the reproductive success of the agent (Maynard \& Price 1973) which could be the expressed in terms of fitness  (Darwin 1872)$\cite{darwin1872origin}$ the fitness in an agent directly influences the frequency of the strategy in the population (Vincent 2005)$\cite{vincent2005evolutionary}$, Maynard and Price take in account three factors to be taken in account: the advantages of winning compared to losing, disadvantage of being seriously injured and disadvantage of wasting time and energy in the contest (Maynard \& Price 1973)$\cite{smith1973lhe}$ this are not usually considered in games but I consider important mentioning. Another very important concept is equilibrium, in some evolutionary games the existence of evolutionary stable strategies (ESS) , I will not yet talk about the mathematical implications of  ESS. Roughly we can consider an ESS as a strategy that predominates in frequency in evolutionary games and that in the case of the emergence of a mutated strategy is not invaded (threatened to be reduce in number) this concept of ESS has similarities with the concept of Nash equilibria as seen by in the sense that both can be ``no-regret'' strategies when in a population a Nash or an ESS is played ``no individual can benefit from unilaterally changing their strategy" (Vincent 2005)$\cite{vincent2005evolutionary}$. According to T.L. Vincent, resistance to invasion is only one of the two notions of an ESS, the second notion is convergence stability which implies that a population evolves to an ESS when its strategy composition is near but not at the ESS mentioned by Eshel in 1983, the implications of this will be discussed later. 

Thinking of how the idea of evolutionary game theory was conceived from behavior of animals, comparing it to the traditional game theory, there are some characteristics that distinguish them from each other. First and perhaps one of the most relevant assumptions in traditional game theory is that every player is rational which means they make rational decisions to maximize their profits, and also they are aware of the possible payoffs of the other players and that other players are rational, also the rational players are aware of the game rules, evolutionary game theory does not make such assumption of rational players, instead the strategies are ‘hard wired’ to them.  Traditional game theory it is about choosing from different strategies looking to optimize the payoffs, whilst evolutionary game theory is to determine strategies that will endure through time. Traditional game theory as said before has a set of strategies from which it can choose, whilst in evolutionary game theory the strategies are already defined given that they are inherited although there can be present some occasional mutations. Also evolutionary game theory there will be groups of players that possess the same set of strategies and the same related payoff from these strategies, in traditional game theory each player has their own set of strategies and their own associated payoffs per strategy (Vincent 2005)$\cite{vincent2005evolutionary}$.  The application of evolutionary game theory in different areas of study has grown, and with this some assumptions change. For example in the biological application players do not choose their strategies and never change them, unlike in the economic application the players are people, who can choose and change their strategies (Samuelson 1997)$\cite{samuelson1998evolutionary}$.
\\\\What is evolutionary game theory?
It can be defined as the combination of some game theoretical concepts, with the concepts of natural selection in evolution. It is a change in the focus from traditional game theory, because the main goal of evolutionary game theory is to observe the stable equilibria and how it changes through time with the interactions between the organisms (players) different behaviours (strategies), instead of only focusing in optimizing outcomes for a single game. Something important to note about the interaction is that in evolutionary biology and evolutionary game theory, the concerning interaction is between individuals of the same species. In this sense we can identify two main approaches to evolutionary game theory (McKenzie 2009)$\cite{mckenzie2009evolutionary}$. The first approach is the ‘static’ approach which is directly derived from the work of Maynard and Price, the main tool for analyzing is the ESS. The second approach through the study of the population dynamics (change in density of existing strategies) and of how the strategies evolve in the model built (McKenzie 2009)$\cite{mckenzie2009evolutionary}$.

\subsection{Evolutionary Stable Strategy (ESS)}
To explain better the concept a symmetric game will be analysed. It is important to mention that the games that we will focus on are two player games. 
When describing ESS the most frequent example used is Hawk-Dove, first used by Maynard and Price in their paper ``The logic of animal conflict'' in 1973 which has been mentioned before. It is worth mentioning that this game has a structure of the well-known ‘prisoner’s dilemma’ game. `Prisoner’s dilemma’ formally presented by Albert Tucker to psychology students in Stanford University in 1950. As we may know ‘prisoner’s dilemma’ is a zero-sum game, in which two individuals who committed a crime are interrogated in separate rooms, and they are not able to exchange information with each other. So they are presented with 2 options (strategies), they can confess or not. And the interaction between the choices each one have go as follow. One confesses and the other does not, both of them confess or neither confess. Each interaction has a pay-off. From this point on I will refer to the players of the games we describe as agents.
First a general definition of the ESS. To illustrate this it will be used a similar approach to the one by Easley \& Kleinberg in ``Networks, Crowds and Markets: Reasoning about a Highly connected World''
We know that a strategy is evolutionarily stable when a population of agents can resist the invasion of emergent mutated agents (new strategy). As mentioned before in evolutionary biology as well as in evolutionary game theory, fitness can be defined as reproductive success (Kleinber 2010)$\cite{easley2010networks}$. Therefore agents with higher fitness value are majority in a population or in time will become majority, whereas agents will low fitness value will be minority and in time with a very high probability will disappear. The fitness value is obtained using the pay-off values from each interaction. With the following table it will be presented a clearer explanation of this.
For a symmetric strategic two-player game we have the following bimatrix.
\begin{center}
Agent 2

Agent 1
\begin{tabular}{|l|c|r|}
\hline
 & X & Y\\ 
\hline
X & a, a & b, c\\
\hline
Y & c, b & d, d\\
\hline
\end{tabular}
\end{center}
\begin{center}
	Figure???: General Symmetric Game
\end{center}
We assume that there exists a large population of agents that always take action X. Now we suppose that within the population appears a small group that take action Y and the   fraction representing the number of agents in this group is $\epsilon$.  Since $\epsilon$ represents the number of agents that choose Y, we can say that 1 - $\epsilon$ is the fraction of the agents that choose X.  We will assume then that the probability of an agent using X encountering another agent at uses X is 1 - $\epsilon$, and with a mutated that uses Y the probability is $\epsilon$.  With these and the values from the table ??? we build the payoff equation for the agent that use strategy X as follows:
\begin{equation}
(1-{\epsilon})a + {\epsilon}b
\end{equation}
And with the same values we build the pay-off equation for the agents using the strategy Y as follows:
\begin{equation}
(1-{\epsilon})c + {\epsilon}d
\end{equation}
For X to be an evolutionary stable strategy, we need:
\begin{equation}
(1-{\epsilon})a + {\epsilon}b > (1-{\epsilon})c + {\epsilon}d
\end{equation}
It should be easy to see that for X to be an ESS a $\>$ c for small values of $\epsilon$, on the other hand when the values of $\epsilon$ are closer to 1, for X to be ESS a = c and b $\>$ d.
Since the pay-offs for each agents are the result from the interactions, we should understand the following.
\begin{itemize}
\item $\textit{For X to be an ESS, the pay-off for interacting with X should be greater or at least equal to the pay-off that an Y gets when interacting with X.}$ 
\item $\textit{Or for X to be an ESS when X and Y get the same pay-off while interacting with X, X needs to get a better pay-off when interacting with Y than Y interacting with Y.}$ 
\end{itemize}
We see that if the previous does not hold, then $(1-{\epsilon})a + {\epsilon}b < (1-{\epsilon})c + {\epsilon}d$ and therefore X is not an ESS.

\subsubsection{Evolutionary stable strategy and Nash equilibrium}
To gain a better understanding of the concepts linking ESS and Nash equilibrium, let us remember what Nash equilibrium is. 
An action (strategy) profile $s^*$, where no player $\textit{i}$ can do better by choosing a different action (strategy) from $s^*_i$, holding player’s $\textit{j}$ action (strategy) $s^*_{j}$ fixed (Osborne 2004)$\cite{osborne2004introduction}$.  Including the notions of rationality we already know.   
Now a similar explanation to the one given in ‘Networks, Crowds, and Markets: Reasoning about a Highly Connected World’ by Easley and Kleinber$\cite{easley2010networks}$.
Looking back at table ???, we can take (X, X) as a Nash equilibrium, therefore we assume X is a best response to both players. And this would mean the following according to the arbitrary values we assumed:
\begin{center}
$\textit{a $\geq$ c}$
\end{center}
The conditions for a strategy to be evolutionary stable are the following:
\begin{center}
$\textit{a $>$ c, or a = c and b $>$ d}$
\end{center}
If $\textit{a $>$ c}$ then X would be a strict Nash equilibrium, on the other hand if $\textit{a = c and b$>$ d}$ the condition is not a strict but still a Nash equilibrium. Also we can draw the following conclusions:

\begin{itemize}
\item $\textit{If X (any strategy s) is not a Nash equilibrium, then X is not evolutionarily stable.}$ 
\item $\textit{If X (any strategy s) is evolutionarily stable, then X is a Nash equilibrium.}$ 
\item $\textit{If X (any strategy s) is a Nash equilibrium, but  $\textit{a = c and b $<$ d}$ then X is not evolutionarily stable.}$
\end{itemize}

Hoping to provide a more understandable explanation of the previous, an example with values will be used and since evolutionary game theory is originated from evolutionary biology, as many literature that defines ESS, we will use an example where the players are animals of a certain type.
\\\\Let us assume that there is a population of wolves all of the same size, but at some point in time a mutation occurs and some bigger wolves result from this. This mutation now represents the fraction $\epsilon$ of the population, and the rest of the population (small size wolves) are represented by 1 - $\epsilon$. These species of wolves as many others hunt in packs and given this cooperative nature the small size wolves share the prey in exactly half. Nevertheless the big sized wolves cannot afford to be so equally sharing, they need a bigger share of the prey since their body needs more nutrients, also given their size they have to make a greater effort when catching the prey. These two issues have made the big wolves aggressive to the other wolves whenever they compete for a share of the prey. The small size wolf avoids conflicts with other wolves, so if a big wolf attacks him after catching the prey they leave, but even with this situation most of the times the small size wolf still has a small share of the prey. A different scenario is given when two big wolves hunt a prey, they will fight each other until they are left seriously injured. This situation is costly for both. The expected pay-offs of these interactions are represented in the following table:
\begin{center}
Wolf size

\begin{tabular}{|l|c|r|}
\hline
 & Small & Big \\ 
\hline
Small & 5, 5 & 1, 7\\
\hline
 Big & 7, 1 & 2, 2\\
\hline
\end{tabular}
\end{center}
\begin{center}
	Figure???: Wolves Hunting
\end{center}

Now we will determine if the small size wolf is an evolutionarily stable strategy, for this we will use the pay-off formulas previously defined, and we have that in this population of wolves, the small size have a pay-off of:
\begin{center}
$(1-{\epsilon})5 + 1{\epsilon} = 5 - 4{\epsilon}$
\end{center}
While the population of bigger wolves have the following expected pay-off:
\begin{center}
$(1-{\epsilon})7 + 2{\epsilon} = 7 - 5{\epsilon}$
\end{center}
For small values of $\epsilon$ we see that the expected pay-off of the big wolves is higher than the small wolves pay-off.  This means that small wolves are not evolutionarily stable.
We can also determine if the big wolves are evolutionarily stable. We now assume that in a population of big wolves, some mutation of small wolves appear in the population in a fraction equal to $\epsilon$. Therefore the population of big wolves is now 1 - $\epsilon$. Now we determine the expected pay-off for big size wolves in this kind of population:
 \begin{center}
$(1-{\epsilon})2 + 7{\epsilon} = 2 + 5{\epsilon}$
\end{center}
And in this population, the mutated small sized wolves will have an expected pay-off of:
\begin{center}
$(1-{\epsilon})1 + 5{\epsilon} = 1 + 4{\epsilon}$
\end{center}
In this population, we can see that the expected pay-off of the big sized wolves is greater than the one for small size wolves. This means that the big size wolf population is evolutionarily stable and a strict Nash Equilibrium.

\subsubsection{How we use it}
Evolution can be thought as a game. According to the *theory of evolution* the existing organisms are the result of many different selections, in which the ancestors resulted the most fitted. These selections were the result of the interaction between the organisms of the same and other species.
\\\\For the purpose of this work, we will be focusing on the interaction between the same species. The model we will use is simplified.


\subsection{Agent-Based Modelling (ABM) and object oriented programming (OOP)}
Generally speaking agents are created entities that represent entities of the real world, and make them interact to study their behavior. One of the first known scientists to show interest in the concept of entities was John von Neumann (Wilensky, Rand 2015)$\cite{wilensky2015introduction}$. He thought in the future it would be useful to have artificial machines that could reproduce autonomously, in order to represent objects such as celestial objects (Wilensky, Rand 2015)$\cite{wilensky2015introduction}$. Given to the suggestion of his colleague Stanislaw Ulam, von Neumann developed a simple model of cellular automaton. In which each cell take one of multiple states, and then the changes in it are based on the history of previous states from the cell and neighboring cells (Janssen, Ostrom 2006)$\cite{janssen2006empirically}$.
\\\\In 1970 Martin Gardner published a game by the british mathematician John Conway. Which was a simplified model of cellular automata applied into what he called “Game of Life” (Janssen, Ostrom 2006)$\cite{janssen2006empirically}$. In which an infinite universe is divided into cells, each cell interact with the neighboring cells (8 cells around it), according to a set of established rules creating complicated patterns according to simple states of the cells, which are alive or dead (Rendell 2001)$\cite{rendell2001turing}$. In 1973 in a joint paper published in Nature (Nature Publishing Group)  by Professor Maynard Smith and George R. Price describe the results of simulations made, which can be considered an example of an agent-based model (Maynard \& Price 1973)$\cite{smith1974theory}$.
\\\\In 1971, 1978 the economist Thomas Schelling used a chessboard in which by moving pennies and dimes he represented what he described in his work ``Models of segregation'' (Janssen, Ostrom 2006)$\cite{janssen2006empirically}$.  Finally another important work using agents were the simulations made by the political scientist Robert Axelrod in 1984 (Janssen, Ostrom 2006)$\cite{janssen2006empirically}$, in which he asked game theorists from different disciplines to submit a strategy which was used in the simulation of a tournament of one of the types of games called `prisoners’ dilemma'. Each strategy interacted with the other strategies, during a number of rounds (Axelrod 1984))$\cite{axelrod1984evolution}$.
\\\\The previously mentioned works are some of the most relevant that involve this agent based modeling. Agent-based models (ABM) are being increasingly used to model complex systems.( Billari, et al 2006))$\cite{billiari2006agent}$. Robert Axelrod in his book `The complexity of Cooperation’ defines ABM as the simulation of agents and their interactions. This type of modeling is different from the traditional type, and it is a type of simulation that is viewed as `bottom-up’ (Axelrod 1997, Billari 2006, Bonabeau 1994, Gilbert 1999)$\cite{axelrod1997complexity}$ used for understanding properties of social systems (Axelrod 1997)$\cite{axelrod1997complexity}$, by representing complex behaviours in the hopes of emulating some specific aspect. There are 3 ways for doing science according to Axelrod, which are induction, deduction and agent-based modeling, the latter starts like deduction with a set of explicit assumptions then it generates simulated data that can be analized inductively. And unlike induction the data produced comes from specific rules rather than real world data (Axelrod 1997)$\cite{axelrod1997complexity}$. Joshua Epstein points ABM as ``a new tool for empirical research" (Epstein, 2006)$\cite{epstein2007agent}$. So it is very important to remember that our final goal with ABM is not to recreate reality, its rather for demonstrating a principal and understand how it works. 
There are at least three main components to take in account when building an ABM, the number of agents with characteristic variables (agents contain data together with methods which act of the data), a set of rules and the environment in which the interaction takes place (Bruun 2004)$\cite{bruun2004agent}$ these will be described in further detail in the description of the simulation. In agent based modeling what matters is not how decisions are made but how the interaction is given (Bruun 2004)$\cite{bruun2004agent}$. 
\\\\A very important step when building a simulation is to choose a programming language.Axelrod mentions that a good agent based model should achieve three goals: validity this is for the program to implement the model correctly, usability which involves the fact that the person building the model and other users can run the program, interpret the output and understand how it works, and the third point is extendability which means the program should be adaptable for new uses in the future (Axelrod 1997)$\cite{axelrod1997complexity}$ . For the type of simulation that will be built, object oriented programming (OOP) is appropriate. In the recent years this language has gained popularity, even that it can be traced for a long way back. Before the concept of OOP was well established, all computing languages were procedural languages, which means that it looked like he program was all contained in one long procedure all data and the logic were presented in a long code.
\\\\OOP's concern is on `objects' (Pokkunuri 1989)$\cite{pokkunuri1989object}$ this objects can be anything one can think of i.e. an action, a thing, a person, a place, of course they have to be well defined. Object is the basic element. Objects possess attributes of procedures and data. Storing the data in variables and responds to messages by executing procedures (in Python called ‘methods’). The program is divided in individual objects (modules) each one can be viewed as an abstract data type. Each of the objects contain their own methods and data (Pokkunuri 1989)$\cite{pokkunuri1989object}$. Communication between objects requesting action can be called ‘message’. The purpose of this is that each object represent parts of whole program, but by breaking it down in modules each can be thought of as a particular action which we can relate in an easier how ideas and actions are structured in our everyday life. As an example we can think of an apple which would be our object and this apple contains attributes such as color, size, weight, etc. but it can also contain methods such as growing, changing color, falling from a tree, etc. This way of conceptualizing ideas in such conventional way in relation to our everyday life make it a perfect candidate for using in simulations. In this project OOP would simplify structuring the ABM, in which it was mentioned before we have so very defined concepts (agents, rules and environments). 
\\\\To be able to work with OOP there is one idea we need to understand properly, which is the difference between class and object. A class can be thought of as a general description or blueprints of something but is not the thing itself.  And what the class is defining are abstract ideas of what was mentioned before, `attributes’ and `methods’, formally a class is defined as a template from which objects are created (Dyke 1989)$\cite{dyke1989object}$. The next concept is object,  would be the ``tangible'' instance of the initial template which is the class(Luna 2012)$\cite{luna2012economic}$. To understand this correctly we can think of the class as the blueprints of something we want to build, and the object is that thing we wanted to build from the blueprints. And when creating an object the abstract ideas from the class, become specific characteristics of the object. A relevant property is inheritance, which allows a class to inherit methods and attributes from another class, this makes the class that inherits a subclass of the class it inherits from, it’s important to mention that this subclass can redefine inherited methods and can add methods that can differentiate it from the class it inherited from (Dyke 1989)$\cite{dyke1989object}$.  Some other relevant properties from OOP are dynamic binding, which is not exclusive for OOP, means that the binding of operator to a particular operation takes place at the run time. Encapsulation is another and it describes the scope of unrestricted reference to the attributes of an object. An object can examine and modify its own attributes, and allows access to its attributes to other objects through accessing functions allowing it to have control over any changes requested from other variables. Data abstraction refers to how any object can be required for any information, and the fact that who requests gets what he/she asked for (Pokkunuri 1989)$\cite{pokkunuri1989object}$. 

\section{Python and genetic algorithm(relating evolutionary game theory)}
We can trace genetic algorithm implementations since...

Genetic algorithm is a type of evolutionary algorithm. It is widely used 

In our code we do not use all the characteristics described for a genetic algorithm. Our agents are created at the beginning of the simulation and are assigned with a specific strategy which they will continue to use until they are eliminated or until the simulation ends. For this the agents created do not posses a chain of phenotypes from which they will occasionally choose and play a different strategy. In our simulation the strategy is assigned randomly and this assures that at least every possible strategy have representatives during the simulation. Additionally we use the concept of asexual reproduction, which means there is no two parents for each "newborn" agent, but instead it is copy of one of the individuals that had the higher utility(fitness) during the generation. The concept of mutation allows us to not always select the individual with the highest payoff to reproduce, but introduces the possibility of another agent with a less efficient strategy to appear. In addition to this concept of mutation

we use another variable which we call exploitation rate. The concept of this is taken from..........................And we implement it as   an additional decision parameter to give more possibility to other less efficient strategies to reproduce. An exploitation rate of 1 (100\%) means that only the agent with highest payoff has a possibility of reproducing itself and a 0 (0\%) means that the whole population is considered a candidate for reproducing itself for the next generation, and this decision is let to be handled by a random choice function. 

\newpage
\section{Software development practices}
\maketitle
\begin{abstract}
In this section a description on how the code was built will be found. The focus used to build the code was a discipline in computer programming called test driven development, some information about this and an example of how to work using this focus will be shown. Version control system will be explained, what is a version control system and the elemental use of the version control system Git will be explained, which is used for interaction between different people that contribute to a single project. An overview of the library built, explaining how the different modules contained interact with each other.

\end{abstract}


\subsection{Test Driven Development}
The idea of OOP goes well with a programming discipline which is being used more commonly, and that is test-driven development(TDD). Is very similar to the idea of test first development (TDF) described by Kent Beck in "Extreme programming", and the main idea is to build a test to assert that the result from a small piece of the application code will be reached, this is very likely to make the programmer think of what is the result she wishes to obtain and very likely as consequence build a very efficient code. Usually a fundamental  step of TDF is described as "write a test that will fail", then under that idea build the functional part of the code that will make that test pass. Test driven development (TDD) follows this same idea, but adds an additional ingredient which is refactoring. In the book "Test-driven development: an example" Kent talks about a TDD "mantra" which is described in 3 simple steps, the first two are the same principles as the ones of TDF. The first step he calls it "Red" and says to write a small test that doesn't work, the second step "Green" is to make the test work (by writing the functional code), and the last step "Refactor" which is getting rid of all possible redundancies in the code without affecting the functional code (Kent 2003)$\cite{beck2003test}$. The testing is usually automated unit testing, the concept of unit testing is useful when using OOP, since the intention of OOP is to keep a clear and functional code with a certain degree of abstraction, unit testing is testing each small component of a program. It is important to say that TDD is intended to test the code in small steps, and not the final application of the code. So in TDD testing and designing the code go together in small steps to produce a final simple and testable code. 

\begin{center}
	\includegraphics[scale=0.35]{TDD}

Figure ???. Basic TDD diagram.
\end{center}

For testing in this code $\textbf{unittest}$ test module contained in the standard Python library was used. $\textbf{unittest}$ from Python supports some important concepts for testing for example test fixture, test case, test suite and test runner (Van Rossum 2003)$\cite{van2003python}$. For the code given that each function of each module was tested we used the smallest unit for testing which is the class $\textbf{TestCase}$. Although there exist many methods for testing in $\textbf{TestCase}$,  for this code the main methods used when testing were:

\begin{itemize}
	\item $\textbf{assertEqual:}$ Asserts an element a is equal to an element  b (a == b) if the values are not equal the test fails.
	\item $\textbf{assertNotEqual:}$ Asserts an element a is not equal to an element b (a !=b) if the values are equal the test fails.
	\item $\textbf{assertTrue:}$ Asserts an argument passed is True (bool(x) is True) test fails if argument is not True. 
	\item $\textbf{assertFalse:}$ Asserts an argument passed is False (bool(x) is False) test fails if argument is not False.
	\item $\textbf{assertIn:}$ Assert an element a is contained in b (a in b) test fails if the element a is not part of b (i.e. a = 3, and b = [1, 5, 6, 8 ,10], value of a (3) is not contained in the list so the test fails).
\end{itemize}

`
\subsection{Version Control}
When teaming with other people with common goals for developing any kind of project, one of the most important elements is communication. There should be a common agreement for a communication channel so every element of the team can be updated with any changes made during the project. 
Developing a project in computer programming is not the exception, and from some years back computer programmers have been using very efficient communication channels. A very useful tool are version control systems (VCS). The main goal from a VCS is to keep track of the development of a project with a very intelligent system that allows people (programmers) to track changes in a project through time. It keeps record of changes that have been made to a project and allows to retrieve the information from those other points in time. VCS basically can create a backups for every changes made to a project, revert specific files, the whole project to a previous state, compare changes over time, see who did last modifications that could be causing a problem, who introduced an issue, when among other (Chacon 2009)$\cite{chacon2009pro}$. Version controlled systems can be local, centralized or distributed, to collaborate in a project nowadays the most popular and perhaps effective choice is to use distributed version control systems where the users fully mirror the set of files or directories under version control commonly known as repository (Chacon 2009)$\cite{chacon2009pro}$. This means that each contributor has a copy of the full repository and if the main server has a malfunction and information is lost, it can be restored by any contributor. 

The VCS used for this project was Git. Git was authored and developed by the creator of Linux kernel, Linus Trovalds in 2005 and many other developers from the Linux community after a commercial break down between the community that developed Linux kernel and the company that provided them DVCS called BitKeeper (Chacon 2009)$\cite{chacon2009pro}$ . 

Git thinks of data like snapshots of a miniature filesystem. Git has a command that is called commit and this helps to record the different snapshots taken in different points of time, one can think of commits as some kind of milestones for the individual and the collective part of the project. An important fact about Git is that since all the history of the project being developed is in each users local disk, the speed to browse around the history of the project is very efficient. This also allows the user to work even if there is no access to the network where the repository is. 

For this project only basic, but still essential features of Git were used. The webpage where the repository for the project was stored is Github, first a user name needs to be created in this webpage and create a repository. Which is relatively easy by following the instructions given in the webpage. It basically asks us to give a name a name and an optional description, the repository can be public or private, the repository for this project is public. It is advised to create a README for describing the project. At the end of the steps one selects create repository and the repository is created. Setting up Git in the pc is not hard, for this I used Git bash which is a build environment shell for Windows and enables Windows to use the Git commands.
There is a lot of information about how to set up Git bash in the web which involves generating a key, by adding a command in bash along with which I added the email address used when signing up in Github in this case and then follow the steps and it is set. 
To start working with a repository from Github either our own or someone else's. We copy an URL that appears on the right bar in the page from the repository we intend to work with. 
This URL was pasted into the Git bash  in the same line after the following command $\textbf{git clone}$ and with this a local clone of the repository is created.
Once the local directory for the project is in the pc we start working on it. Every file that will be included in the project should be in that directory, the state of the files when they are first created is untracked. The following are the commands used during for Git during the project and a short description for each is provided:

\begin{itemize}
	\item $\textbf{git status:}$ Used to see the what the status on the working directory. It will display two sections, one with the staged changes that are ready to be committed and another section displaying the changes that have not been staged, along with a small legend on the left side of the name of the file, that indicates what changes have been done to that file (deleted, modified, new file, etc.)
	\item $\textbf{git add:}$ Used to stage the changes we have made in the directory for the local repository clone specific files. Before using this command all changes that have been made in the directory are considered unstaged. This means that any new file added or any already existing file that has been modified has to be staged with this command in order to include them in the next commit, any changes made that have not been staged with $\textbf{git add}$ will not be considered in the next commit.
	\item $\textbf{git commit:}$ Takes a snapshot of the status of the local cloned repository, of course the files that considers are the ones that $textbf{git add}$ was previously used on. Creates a new point in the history of the project. Once this command is run, it commonly opens a text editor where I introduced the changes that were made in the files that are being changed in this commit.
	\item $\textbf{git diff:}$ Using this command directly displays the content that has been changed in the project since the last   commit and that are not staged for the next commit. If the command is run $\textbf{git diff - - cached}$ it will display the changes that have been staged. If we wish to see staged and unstaged changes together we use $\textbf{git diff HEAD}$ and if there is no interest in watching the specific changes but we want to display more information than just the one presented by $\textbf{git status}$ we add $\textbf{- - stat}$ after the different options we had with $\textbf{git diff [options] - - stat}$ and this displays a summary of the changes.
	\item $\textbf{git fetch [remote-name]:}$ This command pulls all the data from the remote project that is not contained the current copy we hold in our local repository clone. Basically used to update with all the changes other users (contributors) to the project have made, but it does not merge it with the work we have done because this has to be done manually. The name given to the remote repository we created a clone from is usually origin.
	\item $\textbf{git branch:}$  Without any arguments the command gives a list of the existing branches in the local repository.The command followed with the name of the branch $\textbf{git branch (branchname)}$ will create a branch with that given name out of the main project line (master), in the last commit made before the branch. So if we continue working on the master and then we switch to the branch all the changes made to that point will be reverted to the context where the branch was created.  If we wish to delete a local branch we use $\textbf{git branch -d (branchname)}$. To delete a remote branch we use the command $\textbf{git push (remote-name) :(branchname)}$.
	\item $\textbf{git checkout (branchname):}$ This command is used to change between the different existing branches. A way of creating a branch and changing to it at the same time is a shortcut provided by Git $\textbf{git checkout -b (branchname)}$. 
	\item $\textbf{git merge:}$ This command is used to merge the changes that have been made in a branch are complete. And what is usually done is change into the main project line (master) with $\textbf{git checkout (name of the branch we want to merge into)}$ and then use $\textbf{git merge (name of the branch wished to be merged in)}$. This way the changes, if there exists no conflict, will be merged into the main project line (master).
	\item $\textbf{git pull:}$ Basically does first a $\textbf{git fetch}$ immediately followed by a $\textbf{git merge}$ from the tracked branch into the branch we are currently in. This command basically does the same as those two commands, but it can if there are changes that may cause a problem, it might make the process a bit more complex. 

\begin{center}
	\includegraphics[scale=0.7]{gittreec}

Figure ???. Basic Example of a main branch with a branch for being merged.
\end{center}
	
	\item $\textbf{git push:}$ This command helps to update the local changes we have made to the remote repository. And the way to use it is $\textbf{git push (name given to the remote) (name of the local branch)}$. We just need to be careful to not overwrite changes. 
	\item $\textbf{git log:}$  This command displays all the commit messages that have been previously used in the state of the project we currently are in. There is an alternative command that displays how the project has been modified up to the point we currently are in and is $\textbf{git log - - oneline  - - decorate - - graph - - all}$

\begin{center}
	\includegraphics[scale=0.7]{gitlog1}

Figure ???. Basic Example of git log oneline.
\end{center}



\end{itemize}


\subsection{Library (Package)}\label{library_section}
The name chosen for the library is 'Ablearn' and it stands for agent-based learning, the intention is for different people to contribute to this library in the future for it to grow and therefore have a wider range in application at some point in time.

The library is the file that contains all the components of the code. Because of the object oriented properties that Python facilitates, the code is segmented into 6 modules. According to their function the order in describing each is not relevant.

The population module is intended to create agents that will be used in the simulation. The reason of being called agents instead of the classic name player, is because of the intended focus in agent-based modeling as mentioned before. It was build in a very generic way so it can be used, if possible, for any other type of interaction and algorithm in the future. The basic information contained in this class is very generic and it's attributes can be modified when using it. 
This module contains the instructions to create a class named Agent the class has an initializatin(\_\_init\_\_) method that takes as parameters strategies, utility and the possibility to add a label. After the initialization, another method is presented increment\_utility which is set for incrementing the agent’s utility, the criteria for this increment\_utility will be explained in another module.

\begin{itemize}
	\item $\textbf{Strategies:}$ Each created agent will be assigned a strategy
	\item $\textbf{Utility:}$ The utility each created agent generates after each interaction with another agent.
	\item $\textbf{Label:}$ The possibility of adding a label to each created agent, to track their performance.
\end{itemize}

The environment module for this project is the representation of the environment in which the agents will interact and has only two characteristics. It is set to make two agents interact pairing them randomly and it also sets the rules which these paired agents use to interact. Some of the methods contained in this module make use of a module named ‘random’ from python. This module is imported when the environment module is executed. 

Environment module creates a class BiMatrixRandomEnv, named after the characteristics bimatrix and random environment. This class has an initialization (\_\_init\_\_) method that takes as parameters number\_of\_agents and bimatrix, within the initialization the some variables are defined, these variables along with the parameters will now be explained:

\begin{itemize}
\item $\textbf{ Number\_of\_agents:}$ Input by user, total population of agents regardless of the type of agent(i.e. row agent or column agent).
\item $\textbf{Bimatrix}$ Input by user, bimatrix of payoffs(can be symmetric or assymetric).
\item $\textbf{Number\_of\_row\_agents:}$ Result from dividing by “2” the previously input value number\_of\_agents. And gives the number of row agents.
\item $\textbf{Number\_of\_col\_agents:}$ Result from dividing by “2” the previously input value number\_of\_agents. And gives the number of column agents.
\item $\textbf{Number\_of\_row\_strategies:}$ Number of strategies that will be available for row agents. Calculated by counting how many rows the bimatrix has.
\item $\textbf{Number\_of\_col\_strategies:}$ Number of strategies that will be available for column agents. Calculated by counting how many columns the bimatrix has.
\item $\textbf{Row\_strategies:}$ List containing the available strategies for row agents.
\item $\textbf{Col\_strategies:}$ List containing the available strategies for column agents.
\item $\textbf{Row\_agents:}$ Instances of class Agent are created according to the number\_of\_row\_agents. 
\item $\textbf{Col\_agents:}$ Instances of class Agent are created according to the number\_of\_col\_agents.
\end{itemize}

After the initialization, a method `interact’ is defined. This method first defines a variable called ``pairs'' which is assigned to a function `randomly\_pair\_agents’ that will be explained later. It also contains a ``for'' loop this loop within other things contains a variable which is set to a function ``strategies\_to\_utilities'' which will be explained, variables in the ``for'' loop are the following:

\begin{itemize}
\item $\textbf{Ra}$
\item $\textbf{Ca}$
\item $\textbf{Pairs}$
\item $\textbf{Utility}$
\item $\textbf{Agent.increment\_utility}$: The increment utility function is defined in the population model. The structure for in this “for” loop is as follows:
\\ agent.increment\_utility(utility[x]) and what it does is assign the function increment utility to an agent can be ra (row\_agent) or ca (col\_agent), the ``utility'' in parenthesis was assigned in the previous variable, and it is only calling the value with the position x in the list. Given that we only have 2 types of players (we are using a bimatrix) x can be either 0 or 1.
\end{itemize}

Following `interact’ the method previously mentioned `strategies\_to\_utilities’ is defined. This method is in charge of obtaining the specific pair of utilities (assigned to row and column) from the ‘bimatrix’. It then returns these values to the `utility’ variable in the `interact’ method and interact uses it to assign the utilities to each agent.

After `strategies\_to\_utilities’ the method `randomly\_pair\_agents’ is defined. This method is used by `interact’ too, and what it does is that the previously created row and column agents that are contained in lists are randomly selected (one of each type) and then paired so they can interact.



\newpage
\section{Application of the code}

\maketitle
\begin{abstract}
In this section the runs of the program will be discussed. What was consider when running a simulation. An explanation of what games were simulated, and of the results against the known techniques to indicate how the program outputs our expected results when running it with known classic game theory games. A comparison of the results given by the  library 'Axelrod' from python with results from this code 'Ablearn'.
\end{abstract}

\subsection{How the simulation was built.}
After building the code the objective is to determine if the program gives us the expected results. We know that an evolutionary stable strategy is a strategy that survives through time. And as we have mentioned before there are certain conditions that we can verify in order to identify and evolutionary stable strategy, as a reminder those conditions are the following:

\begin{itemize}
\item $\textit{If X (any strategy s) is not a Nash equilibrium, then X is not evolutionarily stable.}$ 
\item $\textit{If X (any strategy s) is evolutionarily stable, then X is a Nash equilibrium.}$ 
\item $\textit{If X (any strategy s) is a Nash equilibrium, but  $\textit{a = c and b $<$ d}$ then X is not evolutionarily stable.}$
\end{itemize}

The following tables represent some of the most well known examples of games in the normal form. 

Perhaps the most known is the the prisoner's dilemma which was mentioned before and it basically has two Nash equilibria strategies one in which both cooperate and the other in which non cooperate.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
 & Cooperate & Defect \\ 
\hline
Cooperate & 3, 3 & 0, 5\\
\hline
 Defect & 5, 0 & 1, 1\\
\hline
\end{tabular}
\end{center}
\begin{center}
	Figure???: Prisoner's Dilemma
\end{center}


Matching pennies which is a pure conflict zero-sum game in which the winner of the game takes all and the loser ends up losing her share. We see that there is no scenario where both players can agree in a strategy. The equilibrium for this game is a mixed Nash equilibrium.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
 & Heads & Tails \\ 
\hline
Heads & 1, -1 & -1, 1\\
\hline
 Tails & -1, 1 & 1, -1\\
\hline
\end{tabular}
\end{center}
\begin{center}
	Figure???: Matching Pennies
\end{center}


The battle of sexes game is a coordination game in which both agents cannot exchange information about what option out of two to choose, they both have a preferred strategy, but if they both choose their preferred strategy they have no payoff from it because they rather choose the same strategy and concur,  even if it has a higher payoff for one of them than the other. In the example from the table there is a representation of preferences, and we should assume that it is a couple trying to decide where to go, the female agent prefers going to the opera, whilst the male agent prefers going to watch football. But we see that if they both end up choosing different strategies from each other they get a payoff of 0. 
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
 & Opera & Football \\ 
\hline
Opera & 1, 2 & 0, 0\\
\hline
 Football & 0, 0 & 2, 1\\
\hline
\end{tabular}
\end{center}
\begin{center}
	Figure???: Battle of Sexes
\end{center}

And the hawk-dove game. This game is often used in  evolutionary game theory. And it represents the 2 strategies an agent can choose,  and the result of the interaction. There is an aggressive strategy which is the hawk strategy and a passive strategy which is the dove. When both agents choose to play hawk, the payoff they get is 0, the explanation is that since they are both aggressive, the possible payoff they could have had from the resource they are competing for is not greater than the cost they pay for playing this strategy against each other. When they both choose dove, they split in equal parts the resource and the payoff they receive is the same for both. When one plays hawk and the other dove, the agent using hawk strategy gets a higher payoff than the one using the dove strategy.    
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
 & Hawk & Dove \\ 
\hline
Hawk & 0, 0 & 3, 1\\
\hline
 Dovel & 1, 3 & 2, 2\\
\hline
\end{tabular}
\end{center}
\begin{center}
	Figure???: Hawk-Dove
\end{center}


The following table indicates how the simulations for prisoner's dilemma were run.
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Num. & Population & Generations& Rounds per Gen. & Death Rate & Mutation Rate & Exploitation Rate\\ 
\hline
1& 1000 & 50 & 50 & 0.1 & 0.1 & 1\\
\hline
2& 1000 & 50 & 25 & 0.1 & 0.1 & 1\\
\hline
3& 1000 & 50 & 5 & 0.1 & 0.1 & 1\\
\hline
4& 1000 & 50 & 5 & 0.5 & 0.1 & 1\\
\hline
5& 1000 & 50 & 5 & 0.9 & 0.1 & 1\\
\hline
6& 1000 & 50 & 5 & 0.9 & 0.5 & 1\\
\hline
7& 1000 & 50 & 5 & 0.9 & 0.9 & 1\\
\hline
8& 1000 & 50 & 5 & 0.1 & 0.5 & 1\\
\hline
9& 1000 & 50 & 5 & 0.1 & 0.9 & 1\\
\hline
10& 1000 & 50 & 5 & 0.1 & 0.1 & 0.5\\
\hline
11& 1000 & 50 & 5 & 0.5 & 0.1 & 0.5\\
\hline
12& 1000 & 50 & 5 & 0.9 & 0.1 & 0.5\\
\hline
13& 1000 & 50 & 5 & 0.1 & 0.5 & 0.5\\
\hline
14& 1000 & 50 & 5 & 0.5 & 0.5 & 0.5\\
\hline
15& 1000 & 50 & 5 & 0.9 & 0.5 & 0.5\\
\hline
16& 1000 & 50 & 5 & 0.1 & 0.9 & 0.5\\
\hline
17& 1000 & 50 & 5 & 0.5 & 0.9 & 0.5\\
\hline
18& 1000 & 50 & 5 & 0.9 & 0.9 & 0.5\\
\hline
19& 1000 & 50 & 5 & 0.1 & 0.1 & 0.1\\
\hline
20& 1000 & 50 & 5 & 0.5 & 0.1 & 0.1\\
\hline
21& 1000 & 50 & 5 & 0.9 & 0.1 & 0.1\\
\hline
22& 1000 & 50 & 5 & 0.1 & 0.5 & 0.1\\
\hline
23& 1000 & 50 & 5 & 0.5 & 0.5 & 0.1\\
\hline
24& 1000 & 50 & 5 & 0.9 & 0.5 & 0.1\\
\hline
25& 1000 & 50 & 5 & 0.1 & 0.9& 0.1\\
\hline
26& 1000 & 50 & 5 & 0.5 & 0.9 & 0.1\\
\hline
27& 1000 & 50 & 5 & 0.9 & 0.9 & 0.1\\
\hline
\end{tabular}
\end{center}
\begin{center}
Table???: Configuration of simulations for prisoner's dilemma.
\end{center}


\subsection{Prisoner's Dilemma}
For Prisoner's dilemma I do not consider relevant to vary the number of generations to test for any changes. This is because the interaction of the agents and their strategies is given in the level of rounds. In other words, the rounds give the proportion of strategies for each generation. A variation in the number of generations may be considered when a clear pattern in the data is not easy to identify. But I do consider important to determine if the number of rounds make a difference in the results. Hence the simulation will be tested  with different number of rounds. It will be done under the conditions 1, 2 and 3 of the table ???(Configuration of simulations).
1.
The simulation starts by assigning the following proportions to row and column agents to the strategies cooperate and defect
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.49 & 0.51\\
\hline
Column Agents & 0.502 & 0.498\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Initial Prisoner’s Dilemma distribution for simulation 1.
\end{center}

In generation 5 it can be seen that one of the strategies is considerably dominating.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.348 & 0.652\\
\hline
Column Agents & 0.24 & 0.76\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 5 Prisoner’s Dilemma distribution for simulation 1.
\end{center}

By generation 10 it can be seen that the dominance of one strategy is almost absolute.

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.054 & 0.946\\
\hline
Column Agents & 0.074 & 0.926\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 10 Prisoner’s Dilemma distribution for simulation 1.
\end{center}

In generation 25 it can be seen that the condition persists and the strategy defect for both types of agents dominates in frequency.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.068 & 0.932\\
\hline
Column Agents & 0.054 & 0.946\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 25 Prisoner’s Dilemma distribution for simulation 1.
\end{center}


In generation 50, the last generation, it be seen that the strategy defect dominates the strategy cooperate.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.01 & 0.99\\
\hline
Column Agents & 0.044 & 0.956\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 50 Prisoner’s Dilemma distribution for simulation 1.
\end{center}

2. We now run the simulation with 25 rounds and we verify the same generations as before

The simulation starts almost with the same proportions as the previous simulation for the strategies of the row and column agents so there is no significant difference at this stage.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.489 & 0.514\\
\hline
Column Agents & 0.524 & 0.476\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Initial Prisoner’s Dilemma distribution for simulation 2.
\end{center}

In generation 5 it can be seen that the proportions changed slightly. For row agents the proportion increased and is very close to the proportion of the column agents in generation 5 from the previous simulation and for the column agents decreased and is very close to the proportion . 
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.25 & 0.75\\
\hline
Column Agents & 0.378 & 0.622\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 5 Prisoner’s Dilemma distribution for simulation 2.
\end{center}

For generation 10 it can be noticed that with respect to simulation 1, an small increase in the defect strategy for row agents, and a small decrease in the same strategy for column agents. 
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.018 & 0.982\\
\hline
Column Agents & 0.094 & 0.906\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 10 Prisoner’s Dilemma distribution for simulation 2.
\end{center}

In generation 25, the proportions increased slightly for strategy defect for both agents. But I do not consider this increment significant.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.045 & 0.955\\
\hline
Column Agents & 0.047 & 0.953\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 25 Prisoner’s Dilemma distribution for this simulation.
\end{center}


In generation 50, both proportions decreased slightly. Since the change in the proportions are very small I do not consider there is significant difference between using 50 or 25 rounds.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.024 & 0.976\\
\hline
Column Agents & 0.08 & 0.92\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 50 Prisoner’s Dilemma distribution for this simulation.
\end{center}

3. We now run the simulation with 5 rounds and compare with the same generations number in the previous 2 simulations.

The simulation starts with proportions for strategies for row and column agents very similar to the previous 2 simulations so there is no significant difference at this stage.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.512 & 0.488\\
\hline
Column Agents & 0.49 & 0.51\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Initial Prisoner’s Dilemma distribution for simulation 3.
\end{center}

 For generation 5 the value for strategy defect for row agents is slightly higher than simulation 1 and slightly smaller than simulation 2. And for the same strategy in column agents compared to simulation 1 it decreased slightly and to simulation 2 it increased. The differences do not appear to be significant.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.35 & 0.65\\
\hline
Column Agents & 0.314 & 0.686\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 5 Prisoner’s Dilemma distribution for simulation 3.
\end{center}

In generation 10 the strategy defect for row agents is slightly smaller with respect to the value in simulation 1 and 2. For the same strategy in column agents it increased slightly when compared to simulation 1 and 2.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.082 & 0.918\\
\hline
Column Agents & 0.07 & 0.93\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 10 Prisoner’s Dilemma distribution for simulation 3.
\end{center}

In generation 25, are slightly smaller for the strategy defect for row agents than previous simulations. For same strategy for column agents is slightly higher than values previous simulations.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.072 & 0.928\\
\hline
Column Agents & 0.054 & 0.972\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 25 Prisoner’s Dilemma distribution for this simulation 3.
\end{center}


In generation 50, value for row agent in strategy defect is slightly higher compared to simulation 2  and smaller from simulation 1. For column agents for the same strategy the value is higher in comparison to the values from the previous simulations.
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.02 & 0.98\\
\hline
Column Agents & 0.028 & 0.972\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 50 Prisoner’s Dilemma distribution for this simulation 3.
\end{center}

After the 3 runs for the simulations, there does not appear to be specific pattern when using the different values for the number of rounds used. For this reason in prisoner's dilemma simulations we will use 5 rounds per generation.

Let us continue to try the different combinations of values that are shown in the table ???(Configuration of simulations). 

4.

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0. & 0.\\
\hline
Column Agents & 0. & 0.\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Initial Prisoner’s Dilemma distribution for simulation .
\end{center}


\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0. & 0.\\
\hline
Column Agents & 0. & 0.\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 5 Prisoner’s Dilemma distribution for simulation .
\end{center}


\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0. & 0.\\
\hline
Column Agents & 0. & 0.\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 10 Prisoner’s Dilemma distribution for simulation .
\end{center}

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0. & 0.\\
\hline
Column Agents & 0. & 0.\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 25 Prisoner’s Dilemma distribution for this simulation .
\end{center}


\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0. & 0.\\
\hline
Column Agents & 0. & 0.\\
\hline
\end{tabular}
\end{center}
\begin{center}
Figure???: Generation 50 Prisoner’s Dilemma distribution for this simulation .
\end{center}

\begin{center}
	\includegraphics[scale=0.5]{pd4}
\end{center}



\section{Results}

\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}