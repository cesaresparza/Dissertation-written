\documentclass{book} 

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self, run, kill_agents,reproduce_agents,random.choice,random.random,copy.deepcopy, assign_strategies,randomly_pair_agents,strategies_to_utilities,interact},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={Genetic, Agents, BiMatrixRandomEnv,__init__,Simulation},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

%Tikz package
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{calc}

% Define block styles
\tikzstyle{envblock} = [rectangle, draw, fill=gray!20, 
    text width=14em, text centered, rounded corners, minimum height=4cm]
\tikzstyle{algblock} = [rectangle, draw, fill=purple!40, 
    text width=14em, text centered, rounded corners, minimum height=4em]
\tikzstyle{ageblock} = [rectangle, draw, fill=orange!20, 
    text width=14em, text centered, rounded corners, minimum height=4em]
\tikzstyle{simblock} = [rectangle, draw, fill=green!30, 
    text width=13.5em, text centered, rounded corners, minimum height=6em]
\tikzstyle{geneblock} = [rectangle, draw, fill=green!30, 
    text width=12em, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex']

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx} 
\graphicspath{ {images/} }
\usepackage{float}

\usepackage{fullpage}
\usepackage[parfill]{parskip}


\newpage
\begin{document}
\thispagestyle{empty}
\title{
{Normal form games}\\
{Cardiff University}\\
{MSc Operation Research and applied statistics}\\
{Dissertation project}\\
}

\author{Cesar Ivan Esparza Soto (esparzasotoci@cf.ac.uk)}
\date{11 of September of 2015}
\maketitle

\chapter*{Abstract}
The literature review can be found in the following section. Some history of Game theory will be mentioned, Nash equilibrium is briefly mentioned but still important since the concept of equilibrium is important to discuss in game theory, simple concept about normal form games, history of evolutionary game theory and an explanation of what evolutionary game theory is and the introduction of the concept of equilibrium in evolutionary game theory which is known as evolutionary stable strategies(ESS), and a brief explanation of the similarities with Nash equilibrium.

\chapter*{Dedication}
To my family

\chapter*{Declaration}
I declare that..

\chapter*{Acknowledgements}
I want to thank..



\newpage
\thispagestyle{empty}
\tableofcontents

\newpage
\thispagestyle{empty}
\listoftables

\newpage
\thispagestyle{empty}
\listoffigures

\newpage
\maketitle
 
\newpage
\pagenumbering{arabic}
\maketitle


The literature review can be found in the following section. Some history of Game theory will be mentioned, Nash equilibrium is briefly mentioned but still important since the concept of equilibrium is important to discuss in game theory, simple concept about normal form games, history of evolutionary game theory and an explanation of what evolutionary game theory is and the introduction of the concept of equilibrium in evolutionary game theory which is known as evolutionary stable strategies(ESS), and a brief explanation of the similarities with Nash equilibrium.



\newpage
\section{Introduction}
\label{Intro:Introduction}
In nature every living creature has specific characteristics that determine their role in their environments. Each living creature from microscopic entities to a blue whale or the tallest tree, posses patterns that allow them to live, some longer than other. We perceive many similarities among living, born at certain point, grow, reproduce and die, that is the path for all. But as similarities exist also differences, living entities in cold environments have different obvious characteristics to others in a hotter environment. But yet despite of those different physical differences that exist I dare to say the most interesting is behaviour. It defines role every existing element has, hunter and prey is the first idea that comes to my mind when talking about roles. But we can go deeper in the nature of these to an we can see that the behaviour is different, but a prey can also be a hunter. That is the essence of behaviour how particular it is to each type of organism although the same goal is always the intention, survival.  As humans we might be one of the most complex creatures in our environment. For some basic needs like eating to survive is not something to be concerned about. And in my mind this is one of the reasons our behaviours have changed, one can say they have evolved, I like to call it adapting. The basic rules of survival  have changed for humans, a big percentage of the humans do not hunt to eat anymore,  some do farming and act as providers for the others. And the key word for me is rules, rules dictate who we are and how we behave  around others. It is bold to say that we all play a game, life, and the rules are not only dictated by our natural environment but also by the social environment. Something I find interesting is asking the question, why do we play for?
\\\\ In this work the intention is not to follow the existential line of previous paragraph, but to give a general framework of a particular branch of game theory and building a program in python to solve simple problems presented in the normal form. The program built in Python will use a simplified genetic algorithm. And the approach of the model is called Agent based modelling, which will be introduced in another chapter. Basic relevant knowledge about evolutionary game theory will be presented. And the Python program will be used to solve some well known normal form games (games is strategic form) like prisoners' dilemma, rock-paper-scissors, matching pennies, and some others. We will also use the program to simulate a an experiment made by the professor in political science Robert Axelrod in 1980, the objective was to see how cooperation evolved in an environment where selfish behaviours exist when interacting through time. The experiment consisted in a tournament where several well-known game theorists participated by providing a strategy that was set to interact with strategies from others, and then determine which strategy was more successful.  

\newpage
\section{Game Theory}
\label{Literature:gt}
Game theory or theory of games is a wildly known theory, which studies the interaction of decisions. This interaction is given to the interdependence between the participants in an environment. Resulting from this there can be mainly two types of interaction cooperative or competitive $\cite{watson2013strategy}$. The whole concept of game theory has a very wide application in many different sciences like economics, companies in a competing market, setting import and exporting tariffs, agreements in wages between employers and employees, auctions, to name some $\cite{gibbons1992primer}$, political sciences Nolan McCarthy mentions examples such as raising more money than other candidates when running for a political position and what to do when the opposition is considered “weak” or “strong”, accepting new policies if the outcomes are not certain,  an example when a jury has to sentence a defendant among others $\cite{mccarty2007political}$, in psychology when studying the behavior of individuals with respect of others, although application of strict assumptions of game theory such as rationality been assumed in the same level for all participants still is a controversial topic for the applying game theory in many areas of social psychology $\cite{rapoport1999game}$ $\cite{colman2003cooperation}$ , biology which will be mentioned further in relation with evolutionary game theory, and other disciplines. Studies that started exploring the potential solution of some games can be traced back to 1713  when James Waldegrave in a letter communicated a mixed solution for a two person game to his colleague Pierre Remond de Montmort $\cite{hykvsova2004several}$ . Since Waldegrave many others did studies that now we relate to game theory $\cite{watson2013strategy}$.  We will start from the french mathematician Emile Borel who in a note in his work ``La th\'{e}orie du jeu et les \'{e}quations, int\'{e}grales \`{a} noyau sym\'{e}trique gauche'' $\cite{borel:1921}$ mentioned:
\\\\ ``The problems of probability and analysis suggest themselves concerning the art of war, or economic or financial speculations, are not without analogy with problems concerning games, though they generally have a higher degree of complication''.
\\\\ In 1924 Borel on his note ``On games that involve chance and the skill of the Players'' $\cite{borel1953games}$ he mentions that ``the study of games that involve at once chance and the skill of the players appears to me similarly able to furnish an opportunity for mathematical research, the applications of which might far surpass the limits of the restricted domain to which this first study is limited. /such research might be extended to very many questions in which psychological unknowns figure along with algebraic unknowns.'' Then Borel continues saying that the only author who had studied problems with this focus was Joseph Bertrand in his ``Calcul des Probalites'' in 1889, distinguishing between mathematical and psychological aspects in an example given of a game of baccarat, but goes on stating why Bertrand study was incomplete $\cite{borel1953games}$.
\\\\ Game Theory was further researched and formally presented by the Hungarian mathematician John von Neumann in 1928 with his work “Theory of Parlor Games”, stating in the introduction that ``… any event – given external conditions and the participants situation (provided the latter are acting of their own free will) – may be regarded as a game of strategy if one looks at the effect it has on the participants.” $\cite{von1928theory}$. And in 1944 John von Neumann and Oskar Morgestern published “Theory of Games and Economic Behavior” in which they established theory of games of strategy as an instrument to study problems in the economic behavior $\cite{von2007theory}$. 
Despite of the great contribution of von Neumann and Morgestern, game theory became widely used after the Doctoral dissertation from John F. Nash was published in 1950. The some of the main arguments from Nash’s work are that he gives the possibility of analyzing games with more n-players, he introduces the concept of non-cooperative games having at least one equilibrium point and gives the idea of a “dynamical” approach to study cooperative games $\cite{nash1951non}$. The last point I mentioned refers to how a cooperative game can be reduced to non-cooperative form, because the idea of a cooperative game established by von Neumann and Morgenstern was that cooperation was given under the assumption described in Nash’s words “players can communicate and form coalitions which will be enforced by and umpire.” $\cite{nash1951non}$ , but he proves this condition is not the only one that would define a cooperative game. The work from Nash opened the possibility of a broader application of the essential concept of game theory, many people have extended the fundamental concepts adopted from Nash’s work and a lot of studies for the development of game theory have been made.

\subsection{Normal form games}\label{second_section}
Watson 2013$\cite{watson2013strategy}$  mentions that there are several mathematical ways to describe games. In game theory there are two most common ways to represent a game called extensive form and normal form. The extensive form which represent in a form of a game tree the different actions that can be taken by each player. Starts with an initial node from which it branches out representing the possible choices and then after each branch another node is placed where a second player has other choices that branch out, until reaching the end of the tree where the payoff for each sequence(following the branches) for the path are represented. But for the purpose of this work, the normal form representation of a game will be used and the basic information about it is the following.
The normal form games, also known as strategic form of games, usually have the following elements:
\begin{itemize}
\item $\textbf{Players}$ A finite set of $\textit{N}$ players.
\item $\textbf{Strategies:}$ A set S$_i$ for each player $\textit{i}$ $\in$ $\textit{N}$. Which as we have discussed represents the action a player chooses.
\item $\textbf{Payoffs:}$ Payoff functions represented for each player represented by $\textit{u$_i$: S$_1$ x S$_2$ x}$ ... $\textit{ x S$_N$}$ $\rightarrow$ $\mathbb{R}$ .Which represent the payoff each player obtain after interacting with each other.
\end{itemize}

For the purpose of this work, the number of players will be $\textit{N}$ = 2 taking the following assumptions from Knight 2014 $\cite{knight2014gt}$ we will represent the game with a $\textbf{ bi-matrix}$. And we will assume that  $\textbf{S$_1$ = \{s$_i$ $|$ 1  $\geq$ i $\geq$ n\}}$ and $\textbf{S$_2$ = \{s$_j$ $|$ 1  $\geq$ j $\geq$ n\}}$ this will be the $\textbf{ bi-matrix}$ for this game :
\begin{table}[h]
\begin{center}
Player 2

Player 1
\begin{tabular}{|l|c|c|c|c|}
\hline
& s$_1$ & s$_2$ & ... & s$_j$\\ 
\hline
r$_1$ & u$_1$(r$_1$,s$_1$), u$_2$(r$_1$,s$_1$) & u$_1$(r$_1$,s$_2$), u$_2$(r$_1$,s$_2$) & ... &  u$_1$(r$_1$,s$_j$), u$_2$(r$_1$,s$_j$\\
\hline
r$_2$ & u$_1$(r$_2$,s$_1$), u$_2$(r$_2$,s$_1$) & u$_1$(r$_2$,s$_2$), u$_2$(r$_2$,s$_2$) & ... &  u$_1$(r$_2$,s$_j$), u$_2$(r$_2$,s$_j$\\
\hline
. & . & . & ... & .\\
. & . & . & ... & .\\
. & . & . & ... & .\\
\hline
r$_i$ & u$_1$(r$_i$,s$_j$), u$_2$(r$_i$,s$_j$) & u$_1$(r$_i$,s$_j$), u$_2$(r$_i$,s$_j$) & ... &  u$_1$(r$_i$,s$_j$), u$_2$(r$_i$,s$_j$)\\
\hline
\end{tabular}
\caption{Normal form game bi-matrix}
\label{tab:normformgame}
\end{center}
\end{table}

We can see that each intersection of row and column have two utility functions. The first utility function represents the payoff for the row player and the second utility function represents the payoff for the column player. Utilities can be of type ordinal, which is used only to rank the alternatives from better to worse, or cardinal which indicates that the value assigned is meaningful and represents the satisfaction of the player $\cite{mccarty2007political}$.


\subsection{Nash Equilibrium}\label{third_section}
John Nash writes in his dissertation ``... an equilibrium point is an n-tuple s so that each player's mixed strategy maximizes his payoff if the strategies of the others are held fixed. Thus each player's strategy is optimal against those of the others.'' $\cite{nash1951non}$.  After this he describes under what circumstances a mixed strategy behaves as a pure strategy. Nowadays the definition of Nash's equilibrium point is known as a Nash equilibrium and we will commonly find the phrase "no regrets" when describing the Nash equilibrium and this is because paraphrasing Nash, no player in the game could have had a better payoff given the action chosen by her opponent. For an N player normal form game Nash equilibrium Knight 2014$\cite{knight2014gt}$ gives the following definition:
\begin{equation}
u_i(\hat{s}) \geq u_i(\bar{s}_i, \wedge{s}_{-i}) \forall i
\end{equation}
 In a two player game, this means each player $\textit{i}$ has a set of strategies $\textit{S}$,  a pair of strategies $(\hat{r}, \hat{s})$ we can define the pure Nash equilibrium as:
\begin{equation}
u_1(\hat{r}, \hat{s}) \geq u_1(r, \hat{s}) \forall r \in S_1   \text{ and }  u_2(\hat{r}, \hat{s}) \geq u_2(\hat{r}, s) \forall r \in S_1
\end{equation}
An "easy" way to find Nash equilibria in games with pure Nash equilibria is by looking at the table in the normal form that holds the strategies and payoffs for each player, and underlining the best response for each player when the other player's strategies are held fixed.
As an example we will use the prisoner's dilemma normal form, and first we hold fixed player's 2 strategies and we underline the best response for player 1 for each of player's 2 strategies.

\begin{table}[h]
\begin{center}
Player 2

Player 1
\begin{tabular}{|l|c|c|}
\hline
 & Cooperate & Defect\\ 
\hline
Cooperate & 3, 3 & 0, 5\\
\hline
Defect & \underline{5}, 0 & \underline{1}, 1\\
\hline
\end{tabular}
\caption{Best responses for player 1}
\label{tab:normformbr1}
\end{center}
\end{table}

In this first table we have that defect is the best response for player 1 in the case where player 2 chooses either of the strategies. 
Now we explore the options holding player 1's strategies fixed and we have the following.

\begin{table}[h]
\begin{center}
Player 2

Player 1
\begin{tabular}{|l|c|c|}
\hline
 & Cooperate & Defect\\ 
\hline
Cooperate & 3, 3 & 0, \underline{5}\\
\hline
Defect & 5, 0 & 1, \underline{1}\\
\hline
\end{tabular}
\caption{ Best responses for player 2}
\label{tab:normformbr2}
\end{center}
\end{table}

And now we only take the strategies that are best responses for both players. Remembering that is the strategy where neither of the players have any reason to change from given that whatever the other player choice is it will always give them the best payoff available for the interaction.
 \begin{table}[h]
\begin{center}
Player 2

Player 1
\begin{tabular}{|l|c|c|}
\hline
 & Cooperate & Defect\\ 
\hline
Cooperate & 3, 3 & 0, 5\\
\hline
Defect & 5, 0 & \underline{1}, \underline{1}\\
\hline
\end{tabular}
\caption{ Nash equilibrium with best responses.}
\label{tab:normformbr3}
\end{center}
\end{table}

\subsection{Evolutionary game theory}\label{EGT}
Evolutionary game theory has its roots in evolutionary biology. Even if this overview is not intended to be focused in biology some remarks from it are worth mentioning. From the 6th edition of ‘Origin of species’ Charles Darwin outlines that in nature there exist many struggles for existence. Some examples are the struggles of a species in the nature, between species and within species. Darwin stresses that the most severe struggle might be within species if they become into competition $\cite{darwin1872origin}$. In origin of species with the idea of all possible interactions of the `organic’ beings mentions that there may be infinite varied diversities of structure for each being under the changing conditions of life, and continues saying that through the course of generations, there can occur variations that can give perhaps a slightly advantage to some beings giving them a higher chance of survival and procreation. The preservation of favourable characteristics and destruction of what he called injurious, he called it natural selection or survival of the fittest $\cite{darwin1872origin}$. I go this far because later on we can think of the concept of fitness, as the ‘utility’ a player has in evolutionary game theory which in general terms can be called the ‘fittest’.

Fisher might have been the first to apply game theory to evolution in his study on sex ratios in 1930 $\cite{pallen2009rough}$ although at this time the formal definition of game theory had not been presented yet. Later in 1961 Lewontin discussed species playing against nature, and said that species should adopt the ``maximin'' strategy if nature presented worse-case scenarios $\cite{sigmund2004maynard}$. In 1967 in an the article ‘Extraordinary sex ratios’ wrote in ‘Science’ William D. Hamilton uses game theory to model local competition and frequency-dependent fitness values $\cite{sigmund2004maynard}$. Perhaps the most important contribution in evolutionary game theory was the one made by Professor John Maynard and Dr. George Price. John Maynard’s attention was first caught by the article ''Antlers, Intraspecific Combat, and Altruism',' written by George Price in 1968 for `Nature’ about ritualized behavior in animal contests $\cite{smith1976evolution}$ unpublished for being too long. Then in 1973 John Maynard and George Price published a joint paper ‘On the logic of animal conflict’ in which the mathematical concept of an evolutionary stable strategy is established, and it applies concepts of game theory to the study of conflicts between animals $\cite{sigmund2004maynard}$.  The idea Maynard and Price presented was that concepts from game theory could be used to characterize eventually stable endpoints in the evolutionary process, with the concept of evolutionary stable strategy $\cite{mcnamara2010evolutionary}$. It can be said that the concept of evolutionary game theory was born with the ideas of John Maynard and George Price.

Nowadays evolution by natural selection can be thought of as a game, where some behavioural patterns (often referred to as phenotypes the equivalent to strategies when related to traditional game theory) from animals are more successful than others $\cite{carmichael2005guide}$. Now I can relate evolutionary game theory with traditional game theory in the some essential concepts. Animals in the biological concept are equivalent to the players (agents) that participate in a game; the environment in which animals interact is comparable to the set of rules that regulate interaction in the traditional form; as mentioned before the heritable phenotypes of animals can be thought of as the strategies that players use in the traditional form; a tricky concept to relate to evolutionary game theory is the one of payoffs (utility) in traditional game theory  for this I will refer to how Maynard and Price define it in ‘The logic of animal conflict’ as the contribution the contest has made to the reproductive success of the agent $\cite{smith1973lhe}$ which could be the expressed in terms of fitness  $\cite{darwin1872origin}$ the fitness in an agent directly influences the frequency of the strategy in the population $\cite{vincent2005evolutionary}$, Maynard and Price take in account three factors to be taken in account: the advantages of winning compared to losing, disadvantage of being seriously injured and disadvantage of wasting time and energy in the contest $\cite{smith1973lhe}$ this are not usually considered in games but I consider important mentioning. Another very important concept is equilibrium, in some evolutionary games the existence of evolutionary stable strategies (ESS) , I will not yet talk about the mathematical implications of  ESS. Roughly we can consider an ESS as a strategy that predominates in frequency in evolutionary games and that in the case of the emergence of a mutated strategy is not invaded (threatened to be reduce in number) this concept of ESS has similarities with the concept of Nash equilibria as seen by in the sense that both can be ``no-regret'' strategies when in a population a Nash or an ESS is played ``no individual can benefit from unilaterally changing their strategy" $\cite{vincent2005evolutionary}$.  

Thinking of how the idea of evolutionary game theory was conceived from behavior of animals, comparing it to the traditional game theory, there are some characteristics that distinguish them from each other. First and perhaps one of the most relevant assumptions in traditional game theory is that every player is rational which means they make rational decisions to maximize their profits, and also they are aware of the possible payoffs of the other players and that other players are rational, also the rational players are aware of the game rules, evolutionary game theory does not make such assumption of rational players, instead the strategies are ‘hard wired’ to them.  Traditional game theory it is about choosing from different strategies looking to optimize the payoffs, whilst evolutionary game theory is to determine strategies that will endure through time. Traditional game theory as said before has a set of strategies from which it can choose, whilst in evolutionary game theory the strategies are already defined given that they are inherited although there can be present some occasional mutations. Also evolutionary game theory there will be groups of players that possess the same set of strategies and the same related payoff from these strategies, in traditional game theory each player has their own set of strategies and their own associated payoffs per strategy $\cite{vincent2005evolutionary}$.  The application of evolutionary game theory in different areas of study has grown, and with this some assumptions change. For example in the biological application players do not choose their strategies and never change them, unlike in the economic application the players are people, who can choose and change their strategies (Samuelson 1997)$\cite{samuelson1998evolutionary}$.
\\\\What is evolutionary game theory?
It can be defined as the combination of some game theoretical concepts, with the concepts of natural selection in evolution. It is a change in the focus from traditional game theory, because the main goal of evolutionary game theory is to observe the stable equilibria and how it changes through time with the interactions between the organisms (players) different behaviours (strategies), instead of only focusing in optimizing outcomes for a single game. Something important to note about the interaction is that in evolutionary biology and evolutionary game theory, the concerning interaction is between individuals of the same species. In this sense we can identify two main approaches to evolutionary game theory $\cite{mckenzie2009evolutionary}$. The first approach is the ‘static’ approach which is directly derived from the work of Maynard and Price, the main tool for analyzing is the ESS. The second approach through the study of the population dynamics (change in density of existing strategies) and of how the strategies evolve in the model built $\cite{mckenzie2009evolutionary}$.

\subsection{Evolutionary Stable Strategy (ESS)}\label{EES}
To explain better the concept a symmetric game will be analysed. It is important to mention that the games that we will focus on are two player games. 
When describing ESS the most frequent example used is Hawk-Dove, first used by Maynard and Price in their paper ``The logic of animal conflict'' in 1973 which has been mentioned before. It is worth mentioning that this game has a structure of the well-known ‘prisoner’s dilemma’ game. `Prisoner’s dilemma’ formally presented by Albert Tucker to psychology students in Stanford University in 1950. As we may know ‘prisoner’s dilemma’ is a game in which two individuals who committed a crime are interrogated in separate rooms, and they are not able to exchange information with each other. So they are presented with 2 options (strategies), they can confess or not. And the interaction between the choices each one have go as follow. One confesses and the other does not, both of them confess or neither confess. Each interaction has a pay-off. From this point on I will refer to the players of the games we describe as agents.
First a general definition of the ESS. To illustrate this it will be used a similar approach to the one by Easley \& Kleinberg in ``Networks, Crowds and Markets: Reasoning about a Highly connected World''
We know that a strategy is evolutionarily stable when a population of agents can resist the invasion of emergent mutated agents (new strategy). As mentioned before in evolutionary biology as well as in evolutionary game theory, fitness can be defined as reproductive success $\cite{easley2010networks}$. Therefore agents with higher fitness value are majority in a population or in time will become majority, whereas agents will low fitness value will be minority and in time with a very high probability will disappear. The fitness value is obtained using the pay-off values from each interaction. With the following table it will be presented a clearer explanation of this.
For a symmetric strategic two-player game we have the following bimatrix.

\begin{table}[h]
\begin{center}
Agent 2

Agent 1
\begin{tabular}{|l|c|r|}
\hline
 & X & Y\\ 
\hline
X & a, a & b, c\\
\hline
Y & c, b & d, d\\
\hline
\end{tabular}
\end{center}
\caption{ General Symmetric Game.}
\label{tab:gensymgame}
\end{table}

We assume that there exists a large population of agents that always take action X. Now we suppose that within the population appears a small group that take action Y and the   fraction representing the number of agents in this group is $\epsilon$.  Since $\epsilon$ represents the number of agents that choose Y, we can say that 1 - $\epsilon$ is the fraction of the agents that choose X.  We will assume then that the probability of an agent using X encountering another agent at uses X is 1 - $\epsilon$, and with a mutated that uses Y the probability is $\epsilon$.  With these and the values from the figure 1.2 we build the payoff equation for the agent that use strategy X as follows:
\begin{equation}
(1-{\epsilon})a + {\epsilon}b
\end{equation}
And with the same values we build the pay-off equation for the agents using the strategy Y as follows:
\begin{equation}
(1-{\epsilon})c + {\epsilon}d
\end{equation}
For X to be an evolutionary stable strategy, we need:
\begin{equation}
(1-{\epsilon})a + {\epsilon}b > (1-{\epsilon})c + {\epsilon}d
\end{equation}
It should be easy to see that for X to be an ESS a $\>$ c for small values of $\epsilon$, on the other hand when the values of $\epsilon$ are closer to 1, for X to be ESS a = c and b $\>$ d.
Since the pay-offs for each agents are the result from the interactions, we should understand the following.
\begin{itemize}
\item For X to be an ESS, the pay-off for interacting with X should be greater or at least equal to the pay-off that an Y gets when interacting with X.
\item Or for X to be an ESS when X and Y get the same pay-off while interacting with X, X needs to get a better pay-off when interacting with Y than Y interacting with Y.
\end{itemize}
We see that if the previous does not hold, then $(1-{\epsilon})a + {\epsilon}b < (1-{\epsilon})c + {\epsilon}d$ and therefore X is not an ESS.
So far we have defined the possibility of having a pure strategy dominating in time. There also exist a case where pure strategies can be played with different probabilities, this will prevent from having a pure strategy equilibria causing an evolutionarily stable mixed strategy. I consider important to mention that when only a pure strategy is played by any entity, it means that it is palyed with a probability of 1 (100\%) , if a mixed strategy is to be played it means that proportion is distributed among the possible effective strategies. In Osborne 2004$\cite{osborne2004introduction}$, we can find a fragment of Maynard Smith's work where he explains that  if randomizing between strategies offers an advantage it can evolve in time, therefore it is possible to be inherited from parents. 

\subsubsection{Evolutionary stable strategy and Nash equilibrium} \label{EESNE}
To gain a better understanding of the concepts linking ESS and Nash equilibrium, let us remember what Nash equilibrium is. 
An action (strategy) profile $s^*$, where no player $\textit{i}$ can do better by choosing a different action (strategy) from $s^*_i$, holding player’s $\textit{j}$ action (strategy) $s^*_{j}$ fixed $\cite{osborne2004introduction}$.  Including the notions of rationality we already know.   
Now a similar explanation to the one given in ‘Networks, Crowds, and Markets: Reasoning about a Highly Connected World’ by Easley and Kleinber$\cite{easley2010networks}$.
Looking back at table ???, we can take (X, X) as a Nash equilibrium, therefore we assume X is a best response to both players. And this would mean the following according to the arbitrary values we assumed:
\begin{equation}
{a \geq c}
\end{equation}
The conditions for a strategy to be evolutionarily stable are the following:
\begin{equation}
{a > c, \text{ or } a = c \text{ and } b > d}
\end{equation}
If $\textit{a $>$ c}$ then X would be a strict Nash equilibrium, on the other hand if $\textit{a = c and b$>$ d}$ the condition is not a strict but still a Nash equilibrium.From Easley 2010 $\cite{easley2010networks}$ we can draw the following conclusions:

\begin{itemize}
\item $\textit{If X (any strategy s) is not a Nash equilibrium, then X is not evolutionarily stable.}$ 
\item $\textit{If X (any strategy s) is evolutionarily stable, then X is a Nash equilibrium.}$ 
\item $\textit{If X (any strategy s) is a Nash equilibrium, but  $\textit{a = c and b $<$ d}$ then X is not evolutionarily stable.}$
\end{itemize}

Hoping to provide a more understandable explanation of the previous, an example with values will be used and since evolutionary game theory is originated from evolutionary biology, as many literature that defines ESS, we will use an example where the players are animals of a certain type.
\\\\Let us assume that there is a population of wolves all of the same size, but at some point in time a mutation occurs and some bigger wolves result from this. This mutation now represents the fraction $\epsilon$ of the population, and the rest of the population (small size wolves) are represented by 1 - $\epsilon$. These species of wolves as many others hunt in packs and given this cooperative nature the small size wolves share the prey in exactly half. Nevertheless the big sized wolves cannot afford to be so equally sharing, they need a bigger share of the prey since their body needs more nutrients, also given their size they have to make a greater effort when catching the prey. These two issues have made the big wolves aggressive to the other wolves whenever they compete for a share of the prey. The small size wolf avoids conflicts with other wolves, so if a big wolf attacks him after catching the prey they leave, but even with this situation most of the times the small size wolf still has a small share of the prey. A different scenario is given when two big wolves hunt a prey, they will fight each other until they are left seriously injured. This situation is costly for both. The expected pay-offs of these interactions are represented in the following table:

\begin{table}[h]
\begin{center}
Wolf size


\begin{tabular}{|l|c|r|}
\hline
 & Small & Big \\ 
\hline
Small & 5, 5 & 1, 7\\
\hline
 Big & 7, 1 & 2, 2\\
\hline
\end{tabular}
\caption{Wolves Hunting}
\label{tab:wolveshunt}
\end{center}
\end{table}

Now we will determine if the small size wolf is an evolutionarily stable strategy, for this we will use the pay-off formulas previously defined, and we have that in this population of wolves, the small size have a pay-off of:
\begin{equation}
(1-{\epsilon})5 + 1{\epsilon} = 5 - 4{\epsilon}
\end{equation}
While the population of bigger wolves have the following expected pay-off:
\begin{equation}
(1-{\epsilon})7 + 2{\epsilon} = 7 - 5{\epsilon}
\end{equation}
For small values of $\epsilon$ we see that the expected pay-off of the big wolves is higher than the small wolves pay-off.  This means that small wolves are not evolutionarily stable.
We can also determine if the big wolves are evolutionarily stable. We now assume that in a population of big wolves, some mutation of small wolves appear in the population in a fraction equal to $\epsilon$. Therefore the population of big wolves is now 1 - $\epsilon$. Now we determine the expected pay-off for big size wolves in this kind of population:
 \begin{equation}
(1-{\epsilon})2 + 7{\epsilon} = 2 + 5{\epsilon}
\end{equation}
And in this population, the mutated small sized wolves will have an expected pay-off of:
\begin{equation}
(1-{\epsilon})1 + 5{\epsilon} = 1 + 4{\epsilon}
\end{equation}
In this population, we can see that the expected pay-off of the big sized wolves is greater than the one for small size wolves. This means that the big size wolf population is evolutionarily stable and a strict Nash Equilibrium.

\subsection{Agent Based Modelling and Object Oriented Programming} \label{ABM}
When building a computer simulations focused in social studies there are two concepts that can helpful. Agent-Based modeling is a good approach to represent the elements that are in someway the subjects of study and object oriented programming (OOP) can help to represent agents with there individual characteristics, given its versatile style.
\subsubsection{Agent-Based Modelling (ABM)}
Generally speaking agents are created entities that represent entities of the real world, and make them interact to study their behavior. One of the first known scientists to show interest in the concept of entities was John von Neumann $\cite{wilensky2015introduction}$. He thought in the future it would be useful to have artificial machines that could reproduce autonomously, in order to represent objects such as celestial objects $\cite{wilensky2015introduction}$. Given to the suggestion of his colleague Stanislaw Ulam, von Neumann developed a simple model of cellular automaton. In which each cell take one of multiple states, and then the changes in it are based on the history of previous states from the cell and neighboring cells $\cite{janssen2006empirically}$.
\\\\In 1970 Martin Gardner published a game by the british mathematician John Conway. Which was a simplified model of cellular automata applied into what he called “Game of Life” $\cite{janssen2006empirically}$. In which an infinite universe is divided into cells, each cell interact with the neighboring cells (8 cells around it), according to a set of established rules creating complicated patterns according to simple states of the cells, which are alive or dead $\cite{rendell2001turing}$. In 1973 in a joint paper published in Nature (Nature Publishing Group)  by Professor Maynard Smith and George R. Price describe the results of simulations made, which can be considered an example of an agent-based model $\cite{smith1974theory}$.
\\\\In 1971, 1978 the economist Thomas Schelling used a chessboard in which by moving pennies and dimes he represented what he described in his work ``Models of segregation'' $\cite{janssen2006empirically}$.  Finally another important work using agents were the simulations made by the political scientist Robert Axelrod in 1984 $\cite{janssen2006empirically}$, in which he asked game theorists from different disciplines to submit a strategy which was used in the simulation of a tournament of one of the types of games called `prisoners’ dilemma'. Each strategy interacted with the other strategies, during a number of rounds $\cite{axelrod1984evolution}$.
\\\\The previously mentioned works are some of the most relevant that involve this agent based modeling. Agent-based models (ABM) are being increasingly used to model complex systems $\cite{billiari2006agent}$. Robert Axelrod in his book `The complexity of Cooperation’ defines ABM as the simulation of agents and their interactions. This type of modeling is different from the traditional type, and it is a type of simulation that is viewed as `bottom-up’ $\cite{axelrod1997complexity}$ used for understanding properties of social systems $\cite{axelrod1997complexity}$, by representing complex behaviours in the hopes of emulating some specific aspect. There are 3 ways for doing science according to Axelrod, which are induction, deduction and agent-based modeling, the latter starts like deduction with a set of explicit assumptions then it generates simulated data that can be analized inductively. And unlike induction the data produced comes from specific rules rather than real world data $\cite{axelrod1997complexity}$. Joshua Epstein points ABM as ``a new tool for empirical research" $\cite{epstein2007agent}$. So it is very important to remember that our final goal with ABM is not to recreate reality, its rather for demonstrating a principal and understand how it works. 
There are at least three main components to take in account when building an ABM, the number of agents with characteristic variables (agents contain data together with methods which act of the data), a set of rules and the environment in which the interaction takes place $\cite{bruun2004agent}$ this will be described in further detail in the description of the simulation. In agent based modeling what matters is not how decisions are made but how the interaction is given $\cite{bruun2004agent}$. 
\\\\A very important step when building a simulation is to choose a programming language. Axelrod mentions that a good agent based model should achieve three goals: validity this is for the program to implement the model correctly, usability which involves the fact that the person building the model and other users can run the program, interpret the output and understand how it works, and the third point is extendability which means the program should be adaptable for new uses in the future $\cite{axelrod1997complexity}$ . For the type of simulation that will be built, object oriented programming (OOP) is appropriate. In the recent years this language has gained popularity, but it can be traced for a long way back. Before the concept of OOP was well established, all computing languages were procedural languages, which means that it looked like the program was all contained in one long procedure all data and the logic were presented in a long code.

\subsubsection{Object Oriented Programming (OOP)} \label{OOP}
OOP's concern is on `objects' $\cite{pokkunuri1989object}$ this objects can be anything one can think of i.e. an action, a thing, a person, a place, of course they have to be well defined. Object is the basic element. Objects possess attributes of procedures and data. Storing the data in variables and responds to messages by executing procedures (in Python called ‘methods’). The program is divided in individual objects (modules) each one can be viewed as an abstract data type. Each of the objects contain their own methods and data $\cite{pokkunuri1989object}$. Communication between objects requesting action can be called ‘message’. The purpose of this is that each object represent parts of whole program, but by breaking it down in modules each can be thought of as a particular action which we can relate in an easier how ideas and actions are structured in our everyday life. As an example we can think of an apple which would be our object and this apple contains attributes such as color, size, weight, etc. but it can also contain methods such as growing, changing color, falling from a tree, etc. This way of conceptualizing ideas in such conventional way in relation to our everyday life make it a perfect candidate for using in simulations. In this project OOP would simplify structuring the ABM, in which it was mentioned before we have so very defined concepts (agents, rules and environments). 
\\\\To be able to work with OOP there is one idea we need to understand properly, which is the difference between class and object. A class can be thought of as a general description or blueprints of something but is not the thing itself.  And what the class is defining are abstract ideas of what was mentioned before, `attributes’ and `methods’, formally a class is defined as a template from which objects are created $\cite{dyke1989object}$. The next concept is object,  would be the ``tangible'' instance of the initial template which is the class$\cite{luna2012economic}$. To understand this correctly we can think of the class as the blueprints of something we want to build, and the object is that thing we wanted to build from the blueprints. And when creating an object the abstract ideas from the class, become specific characteristics of the object. A relevant property is inheritance, which allows a class to inherit methods and attributes from another class, this makes the class that inherits a subclass of the class it inherits from, it’s important to mention that this subclass can redefine inherited methods and can add methods that can differentiate it from the class it inherited from $\cite{dyke1989object}$.  Some other relevant properties from OOP are dynamic binding, which is not exclusive for OOP, means that the binding of operator to a particular operation takes place at the run time. Encapsulation is another and it describes the scope of unrestricted reference to the attributes of an object. An object can examine and modify its own attributes, and allows access to its attributes to other objects through accessing functions allowing it to have control over any changes requested from other variables. Data abstraction refers to how any object can be required for any information, and the fact that who requests gets what he/she asked for $\cite{pokkunuri1989object}$. 

With the created code it will be noticed that individual agents with individual characteristics are created, and they are set to interact in a certain way under determined rules, the final goal will be to have a general perspective of how the interaction of these individual objects leads to a resulting conclusion. In this case we will be able to observe how agents represent different strategies and what is the result of the interaction which will be how one strategy dominates another or how they interact and find a balance. The language chosen for writing the code is Python which is a high level programming language, designed by Guido van Rossum. It serves general programming purposes and it is a very effective object oriented language.

\section{Genetic algorithm and evolutionary game theory} \label{sec:genalg}
Algorithms inspired in evolution for optimization and machine learning, have been the subject of interest for many. After the second world war von Neumann showed great interest in artificial intelligence, his main studies were focused and can be said that originated what now known as cellular automata $\cite{boden2006mind}$. The mathematician Nils Barricelli wrote simulations of evolutionary processes, in the interest of creating artificial life in computers using nature processes. He beccame the first to write a genetic algorithm software, and his first work about it was published in 1954 $\cite{simon2013evolutionary}$. Another person who would use computer programs to simulate evolution was the biologist Alexander Fraser. Fraser was interested in evolution, but in the real world evolution happened to slowly for this reason he decided to use simulations created in digital computers. Fraser wrote in 1957 "Simulation of genetic systems by automatic digital computers" and with these he became the first person to use computer simulations with the purpose of simulation biological evolution $\cite{simon2013evolutionary}$.  Other scientists also used computers to simulate evolutionary processes like Han-Joachim Bremermann his first paper  about it was a published in 1958 entitled " The evolution of intelligence".  George Box, was another scientist who worked in the field. Interesting about Box is that his main interest was not artificial life or evolution, his interest was more in solving real life problems. The first paper he published that can be related to genetic algorithm was in 1957 and the title was "Evolutionary operation: A method for increasing industrial productivity"$\cite{simon2013evolutionary}$. Box states that living things advance in means of two mechanisms: 1. Genetic variability and 2. Natural selection $\cite{golberg1989genetic}$, and in raw terms he associates a new discovery of a process to a mutation and using or discarding different less efficient processes as natural selection. George Friedman outlined in 1956 how a "selective feedback computer" might evolve electrical circuits to maintain internal temperature of a mobile robot $\cite{boden2006mind}$, this work being similar to nowadays genetic algorithm $\cite{simon2013evolutionary}$.In 1966 Lawrence Fogel along with Alvin Owens and Michael Walsh wrote a book about genetic algorithms with the title "Artificial Intelligence through Simulated evolution" $\cite{simon2013evolutionary}$.  In 1960s and 1970s James Holland with students and colleagues from University of Michigan developed the genetic algorithm $\cite{melanie1999introduction}$. Holland's goal was to study the phenomenon of adaptation of processes in natural systems and develop ways in which the mechanics of adaptation might be imported to computer systems. Holland introduced a population-based algorithm with crossover, inversion and mutation $\cite{melanie1999introduction}$.

Genetic algorithm is a type of evolutionary algorithm. In general terms genetic algorithm can be seen as an imitation of biological evolution for problem solving. A genetic algorithm combines the survival of the fittest with  structured yet randomized information exchange. Genetic algorithms are no simple random walk, they exploit historical information to speculate on new search points expecting improved performance. The main research in genetic algorithms revolves around robustness, the balance between eficiency and efficacy for survival in different environments $\cite{golberg1989genetic}$ Genetic algorithms are blind, they perform an effective search only requiring payoff values associated with individual strings $\cite{golberg1989genetic}$.  Melanie 1999 mentions that although there no widely accepted definition for genetic algorithm in the evolutionary - computational community, most methods self called "genetic algorithm" have in common the use of population of chromosomes, selection according to fitness, crossover to produce new offsprings, and random mutation of the new offspring.  

In our code we do not use all the characteristics described for a genetic algorithm. The characteristics for the algorithm resemble more those of evolutionary game theory. The agents are created at the beginning of the simulation and are assigned with a specific strategy which they will continue to use until they are eliminated or until the simulation ends. For this the agents created do not posses a chain of phenotypes from which they will occasionally choose and play a different strategy. In our simulation the strategy is assigned randomly and this assures that at least every possible strategy have representatives during the simulation. Additionally we use the concept of asexual reproduction, which means there is no two parents for each "newborn" agent, but instead it is copy of one of the individuals that had the higher utility(fitness) during the generation. For convenience a constant population number is maintained. The concept of mutation allows us to not always select the individual with the highest payoff to reproduce, but introduces the possibility of another agent with a less efficient strategy to appear. In addition to this concept of mutation. The use a variable called exploitation rate is introduced mainly because of the article by James March "Exploration and Exploitation in Organizational Learning" in which he names two concepts that can or should be taken in account when making a choice in the organizational aspect. But I considered these concepts valid to the context of our experiment, because the simulations made by the code are random, and though randomness should help to cope with any potential bias, the random interactions may favour one strategy over another potentially good potential strategy. March 1991 mentions that exploration includes things captured by terms such as search, variation, risk taking, experimentation, play, flexibility, discovery and innovation: whilst exploitation includes refinement, choice, production, efficiency, selection, implementation and execution$\cite{march1991exploration}$. In raw terms exploration implies looking for another feasible alternative, in the case of our code we can do this in any percentage of the population, or choosing the alternative that appears to generate the highest utility in every run. In the code it is implemented as  an additional decision parameter to give more possibility to other less efficient strategies during a generation to reproduce. An exploitation rate of 1 (100\%) means that only the agent with highest payoff has a possibility of reproducing itself and a 0 (0\%) means that the whole population is considered a candidate for reproducing itself for the next generation, and this decision is let to be handled by a random choice function. For the purpose of this work, we will be focusing on the interaction between the same species. The model we will use is simplified.

\newpage
\section{Software development practices}
\maketitle

In this section a description on how the code was built will be found. The focus used to build the code was a discipline in computer programming called test driven development, a description of what it is will be given. Version control system will be explained, what is a version control system and the elemental use of the version control system Git will be explained, which is used for interaction between different people that contribute to a single project. An overview of the library built, explaining how the different modules contained.




\subsection{Test Driven Development}
The idea of OOP goes well with a programming discipline which is being used more commonly, and that is test-driven development(TDD). Is very similar to the idea of test first development (TDF) described by Kent Beck in "Extreme programming", and the main idea is to build a test to assert that the result from a small piece of the application code will be reached, this is very likely to make the programmer think of what is the result she wishes to obtain and very likely as consequence build a very efficient code. Usually a fundamental  step of TDF is described as "write a test that will fail", then under that idea build the functional part of the code that will make that test pass. Test driven development (TDD) follows this same idea, but adds an additional ingredient which is refactoring. In the book "Test-driven development: an example" Kent talks about a TDD "mantra" which is described in 3 simple steps, the first two are the same principles as the ones of TDF. The first step he calls it "Red" and says to write a small test that doesn't work, the second step "Green" is to make the test work (by writing the functional code), and the last step "Refactor" which is getting rid of all possible redundancies in the code without affecting the functional code $\cite{beck2003test}$. The testing is usually automated unit testing, the concept of unit testing is useful when using OOP, since the intention of OOP is to keep a clear and functional code with a certain degree of abstraction, unit testing is testing each small component of a program. It is important to say that TDD is intended to test the code in small steps, and not the final application of the code. So in TDD testing and designing the code go together in small steps to produce a final simple and testable code. 

\begin{figure}[h]
\begin{center}
	\includegraphics[scale=0.35]{TDD}

\caption{Basic TDD diagram.}
\label{tab:tdddiag}
\end{center}
\end{figure}

For testing in this code $\textbf{unittest}$ test module contained in the standard Python library was used. $\textbf{unittest}$ from Python supports some important concepts for testing for example test fixture, test case, test suite and test runner $\cite{van2003python}$. For the code given that each function of each module was tested we used the smallest unit for testing which is the class $\textbf{TestCase}$. Although there exist many methods for testing in $\textbf{TestCase}$,  for this code the main methods used when testing were:

\begin{itemize}
	\item $\textbf{assertEqual:}$ Asserts an element a is equal to an element  b (a == b) if the values are not equal the test fails.
	\item $\textbf{assertNotEqual:}$ Asserts an element a is not equal to an element b (a !=b) if the values are equal the test fails.
	\item $\textbf{assertTrue:}$ Asserts an argument passed is True (bool(x) is True) test fails if argument is not True. 
	\item $\textbf{assertFalse:}$ Asserts an argument passed is False (bool(x) is False) test fails if argument is not False.
	\item $\textbf{assertIn:}$ Assert an element a is contained in b (a in b) test fails if the element a is not part of b (i.e. a = 3, and b = [1, 5, 6, 8 ,10], value of a (3) is not contained in the list so the test fails).
\end{itemize}

`
\subsection{Version Control}
When teaming with other people with common goals for developing any kind of project, one of the most important elements is communication. There should be a common agreement for a communication channel so every element of the team can be updated with any changes made during the project. 
Developing a project in computer programming is not the exception, and from some years back computer programmers have been using very efficient communication channels. A very useful tool are version control systems (VCS). The main goal from a VCS is to keep track of the development of a project with a very intelligent system that allows people (programmers) to track changes in a project through time. It keeps record of changes that have been made to a project and allows to retrieve the information from those other points in time. VCS basically can create a backups for every changes made to a project, revert specific files, the whole project to a previous state, compare changes over time, see who did last modifications that could be causing a problem, who introduced an issue, when among other $\cite{chacon2009pro}$. Version controlled systems can be local, centralized or distributed, to collaborate in a project nowadays the most popular and perhaps effective choice is to use distributed version control systems where the users fully mirror the set of files or directories under version control commonly known as repository $\cite{chacon2009pro}$. This means that each contributor has a copy of the full repository and if the main server has a malfunction and information is lost, it can be restored by any contributor. 

The VCS used for this project was Git. Git was authored and developed by the creator of Linux kernel, Linus Trovalds in 2005 and many other developers from the Linux community after a commercial break down between the community that developed Linux kernel and the company that provided them DVCS called BitKeeper $\cite{chacon2009pro}$ . 

Git thinks of data like snapshots of a miniature filesystem. Git has a command that is called commit and this helps to record the different snapshots taken in different points of time, one can think of commits as some kind of milestones for the individual and the collective part of the project. An important fact about Git is that since all the history of the project being developed is in each users local disk, the speed to browse around the history of the project is very efficient. This also allows the user to work even if there is no access to the network where the repository is. 

For this project only basic, but still essential features of Git were used. The webpage where the repository for the project was stored is Github, first a user name needs to be created in this webpage and create a repository. Which is relatively easy by following the instructions given in the webpage. It basically asks us to give a name a name and an optional description, the repository can be public or private, the repository for this project is public. It is advised to create a README for describing the project. At the end of the steps one selects create repository and the repository is created. Setting up Git in the pc is not hard, for this I used Git bash which is a build environment shell for Windows and enables Windows to use the Git commands.
There is a lot of information about how to set up Git bash in the web which involves generating a key, by adding a command in bash along with which I added the email address used when signing up in Github in this case and then follow the steps and it is set. 
To start working with a repository from Github either our own or someone else's. We copy an URL that appears on the right bar in the page from the repository we intend to work with. 
This URL was pasted into the Git bash  in the same line after the following command $\textbf{git clone}$ and with this a local clone of the repository is created.
Once the local directory for the project is in the pc we start working on it. Every file that will be included in the project should be in that directory, the state of the files when they are first created is untracked. To see the basic Git commands with a short description used during this work go to APENDIX A Git Commands.

\subsection{Library (Package)}\label{library_section}
The name chosen for the library is 'Ablearn' and it stands for agent-based learning, the intention is for different people to contribute to this library in the future for it to grow and therefore have a wider range in application at some point in time.

The library is the file that contains all the components of the code. Because of the object oriented properties that Python facilitates, the code is segmented into 6 modules. According to their function the order in describing each is not relevant.

\subsubsection{Population module}
The population module is intended to create agents that will be used in the simulation. The reason of being called agents instead of the classic name player, is because of the intended focus in agent-based modeling as mentioned before. It was build in a very generic way so it can be used, if possible, for any other type of interaction and algorithm in the future. The basic information contained in this class is very generic and it's attributes can be modified when using it. 
This module contains the instructions to create a $\pythoninline{class Agent}$ the class has an initializatin($\pythoninline{__init__}$) method that takes as parameters strategies, utility and the possibility to add a label. After the initialization, another method is presented increment\_utility which is set for incrementing the agent’s utility, the criteria for this increment\_utility will be explained in another module.

\begin{figure}[H]
\begin{center}
\begin{itemize}
	\item $\pythoninline{Strategies:}$ Each created agent will be assigned a strategy
	\item $\pythoninline{Utility:}$ The utility each created agent generates after each interaction with another agent.
	\item $\pythoninline{Label:}$ The possibility of adding a label to each created agent, to track their performance.
\end{itemize}
\begin{tikzpicture}
    \node [ageblock] (agent) at (0,0) [draw, fill=orange!80] {\textbf{Agent} \\ 
	``This class is for creating empty agents, with the attributes: strategies, utility and label(optional)''};
    \node [ageblock, below of=agent, node distance=5cm](increment)[draw] {\textbf{Increment utility} \\
	``This method will increment the accumulated utility for each agent in an amount that will be determined by the payoff each 				type of agent gets in an interaction''};
    % Draw edges   
    \path[line,dashed](agent) -- (increment);       

\end{tikzpicture}
\caption{ Diagram of population module.}
\label{fig:diagage}
\end{center}
\end{figure}
\subsubsection{Environments module}
The environments module for this project is the representation of the environment in which the agents will interact.The environment created within this module for this project is $\pythoninline{bimatrix\_random\_environment}$ and has only two characteristics. It is set to make two agents interact pairing them randomly and it also sets the rules which these paired agents use to interact. Some of the methods contained in this module make use of a module named ‘random’ from python. This module is imported when the environment module is executed. 

$\textbf{Bimatrix\_random\_environment}$ creates a $\pythoninline{class BiMatrixRandomEnv}$, named after the characteristics bimatrix and random environment. This class has an initialization ( $\pythoninline{\_\_init\_\_}$) method that takes as parameters $\pythoninline{number\_of\_agents}$ and $\pythoninline{bimatrix}$, within the initialization some variables are defined, these variables along with the parameters will now be explained:

\begin{itemize}
\item $\pythoninline{ number\_of\_agents:}$ Input by user, total population of agents regardless of the type of agent(i.e. row agent or column agent).
\item $\pythoninline{bimatrix}$ Input by user, bimatrix of payoffs(can be symmetric or assymetric).
\item $\pythoninline{number\_of\_row\_agents:}$ Result from dividing by “2” the previously input value number\_of\_agents. And gives the number of row agents.
\item $\pythoninline{number\_of\_col\_agents:}$ Result from dividing by “2” the previously input value number\_of\_agents. And gives the number of column agents.
\item $\pythoninline{number\_of\_row\_strategies:}$ Number of strategies that will be available for row agents. Calculated by counting how many rows the bimatrix has.
\item $\pythoninline{number\_of\_col\_strategies:}$ Number of strategies that will be available for column agents. Calculated by counting how many columns the bimatrix has.
\item $\pythoninline{row\_strategies:}$ List containing the available strategies for row agents.
\item $\pythoninline{col\_strategies:}$ List containing the available strategies for column agents.
\end{itemize}

After the initialization, a method $\pythoninline{interact}$ is defined. This method first defines a variable called $\pythoninline{pairs}$ which is assigned to a function $\pythoninline{randomly\_pair\_agents}$ that will be explained later. It also contains a $\pythoninline{``for'' loop}$ this loop within other things contains a variable which is set to a function $\pythoninline{strategies\_to\_utilities}$ which will be explained, variables in the $\textbf{``for'' loop}$ are the following:

\begin{itemize}
\item $\pythoninline{ra}$: Used in the code during for loops in exchange of $\pythoninline{row\_agents}$.
\item $\pythoninline{ca}$: Used in the code during for loops in exchange of $\pythoninline{col\_agents}$.
\item $\pythoninline{pairs}$: Variable created to represent the group of paired $\pythoninline{row\_agents}$ with $\pythoninline{col\_agents}$.
\item $\pythoninline{utility}$: Variable used to obtain the utilities resulting from the interaction from each pair of agents ($\textbf{row\_agents and col\_agents}$).
\item $\pythoninline{Agent.increment\_utility}$: The increment utility function is defined in the population model. The structure for in this “for” loop is as follows:
\\ agent.increment\_utility(utility[x]) and what it does is assign the function increment utility to an agent can be $\pythoninline{ra (row\_agent)}$ or$\pythoninline{ ca (col\_agent)}$, the $\pythoninline{utility}$ in parenthesis was assigned in the previous variable, and it is only calling the value with the position x in the list. Given that we only have 2 types of players (we are using a bimatrix) x can be either 0 or 1.
\end{itemize}

Following $\pythoninline{interact}$ the method previously mentioned $\pythoninline{strategies\_to\_utilities}$ is defined. This method is in charge of obtaining the specific pair of utilities (assigned to row and column) from the $\pythoninline{bimatrix}$. It then returns these values to the $\pythoninline{utility}$ variable in the $\pythoninline{interact}$ method and interact uses it to assign the utilities to each agent.

After $\pythoninline{strategies\_to\_utilities}$ the method $\pythoninline{randomly\_pair\_agents}$ is defined. This method is used by $\pythoninline{interact}$ too, and what it does is that the previously created row and column agents that are contained in lists are randomly selected (one of each type) and then paired so they can interact.

\begin{figure}[H]
\begin{tikzpicture}[node distance = 2cm, auto]
    \node [envblock, node distance=4cm] (bimatrix) [draw, fill=gray!80,text=white] {\textbf{BiMatrixRandomEnv} \\
    ``This class is the type of environment in which the row agents and column agents will interact, which is a bimatrix with random interaction. The attributes the class takes are the number of agents and the bimatrix of the resulting payoffs from interactions.''};
    \node [envblock, below of=bimatrix, node distance=6cm] (interact) [draw] {\textbf{interact} \\
	``With this method, agents interact and get a resulting utility from the interaction''};
    \node [envblock, left of=interact, node distance=6cm](utilities)[draw] {\textbf{strategies\_to\_utilities} \\
	``This method determines the utility from each interaction of paired of agents''};
    \node [envblock, right of=interact, node distance=6cm] (pairs) [draw] {\textbf{randomly\_pair\_agents} \\
	``With this method random row agents are paired with random column agents, to then interact in the interact method.''};  
    % Draw edges
    \draw [->, thick, black!100] (bimatrix) edge [out=270, in=90] node[left] {1}  (interact);
    \draw [->, thick, dotted, black!100] (interact) edge [out=110, in=90]node[above] {4} (utilities);
    \draw [->, thick,dotted, black!100] (interact) edge [out=70, in=90] node[above] {2} (pairs);
    \draw [->, thick, dotted, black!100] (utilities) edge [out=270, in=250] node[below] {5} (interact);
    \draw [->, thick,dotted, black!100] (pairs) edge [out=270, in=290] node[below] {3} (interact);
 \end{tikzpicture}
\caption{ Diagram of environments module.}
\label{fig:diagenv}
\end{figure}



\newpage
\subsubsection{Algorithms module}
The $\textbf{algorithms module}$ is a module that is meant to contain different algorithms by which the created $\textbf{agents}$ from $\textbf{population module}$ will be processed. For this project $\textbf{algorithms module}$ contains a $\textbf{genetic algorithm}$.
The  $\textbf{genetic algorithm}$ is named  $\textbf{genetic}$ and it creates a $\pythoninline{class Genetic}$. In the initialization ($\pythoninline{\_\_init\_\_}$) takes the following parameters:

\begin{itemize}
	\item $\pythoninline{generations:}$ Are the number of generations (times) the whole code will be run.
	\item $\pythoninline{rounds\_per\_generation:}$ Are the number of times the interaction of the agents will be given within each generation. 
	\item $\pythoninline{death\_rate:}$ Represents the proportion of agents that will be eliminated after each generation has run.
	\item $\pythoninline{mutation\_rate:}$ This parameter is given as a limit for agents with the highest accumulated utility not being reproduced. A $\pythoninline{mutation\_rate}$ value is established and is compared against a $\pythoninline{random.random()}$ number. Every time the $\pythoninline{mutation\_rate}$ is higher than the value given by the $\pythoninline{random.random()}$ a mutation will occur. This comparison occurs every time a new agent is created when all the interactions within a generation have happened. This means that each new agent is passed under this condition. The numbers given by $\pythoninline{random.random()}$ are decimal numbers, which means that the range for $\pythoninline{mutation\_rate}$ should also be in decimal numbers from $\textbf{ 0 to 1}$.
	\item $\pythoninline{exploitation\_rate:}$ Is a variable introduced to determine if the concept of $\textit{exploitation or exploration}$ will be used. Where a value of 1 (100$\%$) represents a total exploitation, this means reproducing only the agent with the highest utility for each type of agent ($\pythoninline{row\_agents}$ and $\pythoninline{col\_agents}$) and a value of 0 (0$\%$) is used to consider the whole population of agents.
\end{itemize}

Following the initialization ($\pythoninline{\_\_init\_\_}$), the method $\pythoninline{assign\_strategies}$ is found. This method takes parameters $\textbf{agents}$, which according to the type of agent in turn will be represented by $\pythoninline{row\_agents}$ or $\pythoninline{col\_agents}$  and the parameter $\pythoninline{agents\_strategies}$, also which according to the type of agent in turn will be represented by $\pythoninline{row\_strategies}$ or $\pythoninline{col\_strategies}$. This method is used after the different types of agents are created in the $\pythoninline{initialization}$ of the $\pythoninline{class BiMatrixRandomEnv}$ of the module $\pythoninline{environments}$. The $\pythoninline{assign\_strategies}$ method assigns a random strategy for each agent according to the strategies available given their type through a $\textbf{``for'' loop}$ and $\pythoninline{random.choice(agents\_strategies)}$ this random choice selects from the $\pythoninline{row\_strategies}$ or $\pythoninline{col\_strategies}$ randomly according to the type of agent.

After the $\pythoninline{assign\_strategies}$ method, comes a method named $\pythoninline{kill\_agents}$. This method takes as parameters $\pythoninline{agents, number\_of\_agents, number\_of\_x\_agents}$, and $\pythoninline{agents\_strategies}$. In this method a variable $\pythoninline{number\_of\_deaths\_per\_generation}$ is created, which determines the number of agents that will be eliminated for each type of agent group ($\pythoninline{row\_agents}$ or $\pythoninline{ col\_agents}$) it does so by taking the integer value from the product of the multiplication of the variables $\pythoninline{death\_rate}$ \* $\pythoninline{number\_of\_x\_agents}$(this last variable represents the total number of row\_agents or col\_agents).  The value $\pythoninline{number\_of\_deaths\_per\_generation}$ is used in a $\textbf{``while'' loop}$ to eliminate the agents while the condition is not satisfied. Within the ``while'' loop the agents ($\pythoninline{row\_agents}$ or $\pythoninline{col\_agents)}$  are sorted according to their accumulated $\pythoninline{utility}$ from smallest to highest and the one with the lowest accumulated $\pythoninline{utility}$ is deleted from the list of the corresponding type of agents.

After the method $\pythoninline{kill\_agents}$ comes the method $\pythoninline{reproduce\_agents}$. This method takes as parameters $\pythoninline{agents, number\_of\_agents, number\_of\_x\_agents}$ and $\pythoninline{agents\_strategies}$. A variable $\pythoninline{choice}$ is created by taking the integer value of ($\pythoninline{((number\_of\_agents / 2) \*  (exploitation\_rate - 1) + 0.05) -1}$ this value is used within the ``while'' loop that follows. The ``while'' loop used in this method works while the existing number of row or col agents is less than the number of row or col agents created at the beginning of the simulation. Within this loop, the agents are sorted according to their accumulated $\pythoninline{utility}$ values from low to high, and a $\pythoninline{copy.deepcopy}$  function is used to create a copy of an agents chosen with $\pythoninline{random.choice(agents[choice:])}$ where the variable $\pythoninline{choice}$ will indicate the start of the range of agents that can be chosen within the list of agents. After this within the same loop and ``if'' statement is presented which condition is that if the $\pythoninline{mutation\_rate}$ value is greater than a $\pythoninline{random.random()}$ number, the agent that will be reproduced can be any agent (row or col agent) with any value of accumulated $\pythoninline{utility}$ except for the agent (row or col agent) with the highest accumulated $\pythoninline{utility}$  value of that generation.
\begin{figure}[H]
\begin{tikzpicture}
    \node [algblock] (genetic) at (0,0) [draw, fill=purple!80, text=white] {\textbf{Genetic} \\ 
	``This class assign strategies for the new agents, eliminates and creates agents. The initial values it takes are the following''};
    \node [algblock, below of=genetic, node distance=5cm](kill)[draw] {\textbf{kill\_agents} \\
	``This method eliminates a determined number of agents with the lowest accumulated utility each generation''};
    \node [algblock, right of=kill, node distance=6cm](repro)[draw] {\textbf{reproduce\_agents} \\
	``This method maintains the number of agents (row and column agents) in the number determined at the beginning of the 			simulation, by creating new agents with certain characteristics established by the mutation rate and the exploitation rate''};
    \node [algblock, left of=kill, node distance=6cm](assign)[draw] {\textbf{assign\_strategies} \\
	``This method randomly assigns strategies to the agents created (row and column agents)''};
    % Draw edges   
    \path[line](genetic) edge         node[left] {1} (assign);
    \path[line](genetic) edge         node[left]{2} (kill);
    \path[line](genetic) edge         node[right] {3} (repro);         
\end{tikzpicture}
\caption{Diagram of algorithms module.}
\label{fig:diagalg}
\end{figure}



\subsubsection{Simulation module}
The $\textbf{simulation module}$ is a module that is makes all the other modules interact with each other. For this project $\textbf{simulation module}$ is named $\textbf{simulation}$  and it creates a class $\textbf{Simulation}$. In the initialization ($\textbf{\_\_init\_\_}$) takes the parameters used in the other modules in addition to another parameter named $\textbf{tog}$ which stands for ``type of game''. There are 7 predetermined games within the code, which can be selected by typing the correct value of $\textbf{tog}$. $\textbf{tog}$ and the other parameters taken by the class $\textbf{Simulation}$ have the following characteristics:

\begin{itemize}
	\item $\textbf{tog:}$ Stands for ``type of game'' and it serves to pick one of the available choices of bimatrix that have been coded in. The values that can be selected have to go between single quotation marks and are the following: $\textbf{`pd'}$ an example of prisoner's dilemma, $\textbf{`psr'}$ an example of the game paper-scissors-rock, $\textbf{`mp'}$ an example of matching pennies, $\textbf{`bos'}$ an example of battle of sexes game, $\textbf{`hd'}$, $\textbf{`sh'}$ an example of stag hunt game, $\textbf{`cs'}$ an example of choosing sides game and $\textbf{`axl'}$ an example emulating the first tournament from Robert Axelrod with 8 out of the 14 original strategies. This strategies are taken from the axelrod package available for python and with the following strategies: $\textit{Tit for Tat, Grofman, Shubik, Grudger, Davis, Feld, Joss, Tullock and a Random strategy}$.
	\item$\textbf{na:}$ Stands for the value of $\textbf{number\_of\_agents}$ explained in the $\textbf{bimatrix\_random\_environment}$ module.
	\item $\textbf{ge:}$ Stands for the value of $\textbf{generations}$ explained in the $\textbf{genetic}$ algorithm module. And is the number of generations (times) the whole code will be run.
	\item $\textbf{rpg:}$ Stands for the value of $\textbf{rounds\_per\_generation}$ explained in the $\textbf{genetic}$  algorithm module. Is the number of times the interaction of the agents will be given within each generation. 
	\item $\textbf{dr:}$ Stands for $\textbf{death\_rate}$ explained in the $\textbf{genetic}$ algorithm module. And represents the proportion of agents that will be eliminated after each generation has run.
	\item $\textbf{mr:}$ Stands for $\textbf{mutation\_rate}$ explained in the $\textbf{genetic}$ algorithm module. And this parameter is given as a limit for agents with the highest accumulated utility not being reproduced. A $\textbf{mutation\_rate}$ value is established and is compared against a $\textbf{random.random()}$ number. Every time the $\textbf{mutation\_rate}$ is higher than the value given by the $\textbf{random.random()}$ a mutation will occur. This comparison occurs every time a new agent is created when all the interactions within a generation have happened. This means that each new agent is passed under this condition. The numbers given by $\textbf{random.random()}$ are decimal numbers, which means that the range for $\textbf{mutation\_rate}$ should also be in decimal numbers from $\textbf{ 0 to 1}$.
	\item $\textbf{er:}$ Stands for $\textbf{exploitation\_rate}$ explained in the $\textbf{genetic}$ algorithm module. And it is a variable introduced to determine if the concept of $\textit{exploitation or exploration}$ will be used. Where a value of 1 (100$\%$) represents a total exploitation, this means reproducing only the agent with the highest utility for each type of agent ($\textbf{row\_agents and col\_agents}$) and a value of 0 (0$\%$) is used to consider the whole population of agents.
\end{itemize}

In the initialization (\_\_init\_\_) method an instance of the class $\textbf{Genetic}$ from the genetic module is created, an instance for the class $\textbf{Agents}$ from the population module is created also and when selecting the $\textbf{tog}$ value, an instance of the class $\textbf{BiMatrixRandomEnv}$ from the environment module is created. After the instances are created within the initialization method, the function $\textbf{ga.assign\_strategies()}$ from the genetic algorithm module is used to assign the values for the strategies for each type of agent ($\textbf{row\_agents and col\_agents}$) that have been created through the instance of the module environment.After pair of lists with the names $\textbf{row\_accumulated\_strategies}$ and $\textbf{col\_acummulated\_strategies}$ are created respectively and the purpose for each list is the following:

\begin{itemize}
	\item$\textbf{row\_accumulated\_strategies}$: Creates a list, that within itself contains a certain number of individual lists according to the length of $\textbf{row\_strategies}$ which is the number of strategies available for the $\textbf{row\_agents}$ obtained from the module environment.
	\item$\textbf{col\_accumulated\_strategies}$: Creates a list, that within itself contains a certain number of individual lists according to the length of $\textbf{col\_strategies}$ which is the number of strategies available for the $\textbf{col\_agents}$ obtained from the module environment.

\end{itemize}

After the initialization method has created the different instances from the other modules, a method with the name $\textbf{run(plot=False)}$ is presented. This method is used to start running the whole simulation (all the modules). And this method take an attribute $\textbf{plot}$ which is set by default as $\textbf{False}$, if using the method $\textbf{run(plot=True)}$, the simulation will plot the results from each generation in a graph. The graph is produced with the module $\textbf{matplotlib}$ from python and the functions used are $\textbf{pyplot and cm(color map)}$. The graph is set in interactive mode by the function $\textbf{ion( )}$. The limit for the y axis is set $\textbf{ylim(0, 1)}$, the limit for the x axis is the number of $\textbf{generations}$. The x axis is labeled $\textbf{xlabel(``Generations")}$ and the y axis is labeled $\textbf{ylabel(``Probability'')}$. For the coloring the lines representing the different strategies for each type of agents color map is used for the $\textbf{row\_accumulated\_strategies[ ]}$ the color map copper is used and for the $\textbf{col\_accumulated\_strategies[ ]}$ the color map winter is used, how these $\textbf{accumulated\_strategies}$ values were obtained will be explained later, the thing to know about them is that each of them represent the proportion within a type of agent that is using a certain strategy. The graph indicates how the population of the different types of agents with strategies varied through time, the value of interest from this is the strategy the types of agents are using.

After the code for creating the graph a function $\textbf{generations\_passing( )}$ is presented, this function takes attributes $\textbf{generations}$, $\textbf{rounds\_per\_generation}$, $\textbf{number\_of\_agents}$, $\textbf{row\_agents}$, $\textbf{col\_agents}$, and $\textbf{plot}$ and takes us to the next method.

After the $\textbf{run}$ method, there is a method $\textbf{generations\_passing}$ that takes attributes $\textbf{generations}$,  $\textbf{row\_agents}$, $\textbf{col\_agents}$, $\textbf{number\_of\_agents}$, $\textbf{ plot}$. In this method there is a ``for'' loop that loops until the $\textbf{generations}$ value is reached, within this loop exist two other ``for'' loops.
 he first one loops around the $\textbf{row\_agents}$ and  $\textbf{col\_agents}$ setting for each agent  the $\textbf{utility}$ value to 0. 
The second ``for'' loop loops according to the $\textbf{number\_of\_rounds}$ chosen at the beginning of the simulation. Within this loop we find the function $\textbf{interact( )}$ from the module environment, explained before, which in general terms makes the $\textbf{row\_agents and col\_agents}$ interact with each other and as a result obtains the payoff for each type of agent and an attributed $\textbf{utility}$. The loop from the rounds causes this process to repeat several times and when the last round is played all the different agents have an accumulated utility product of the repeated interactions.
When the second loop concludes a function $\textbf{distributions( )}$ follows. This function is for the method $\textbf{distributions}$ that takes attributes $\textbf{number\_of\_agents}$, $\textbf{ row\_agents}$, $\textbf{col\_agents}$, $\textbf{row\_strategies}$, $\textbf{col\_strategies}$, and$\textbf{ plot}$.

Before the method $\textbf{distributions}$ there is a method $\textbf{proportion\_classified\_strategies}$ that takes attributes $\textbf{agents and strategies}$. This method will do some calculations when the code is using the method $\textbf{distributions}$.   The method first creates a varibale $\textbf{frequency\_per\_strategy\_per\_agent}$ with an empty list. Then it has a ``main'' ``for'' loop that loops around the variable $\textbf{strategies}$ (this variable can be $\textbf{row\_strategies}$ or $\textbf{col\_strategies}$ according to which values are being used) of the $\textbf{agent}$ in turn and appends a value of 0 to the list. Then an ``secondary'' loop is contained within the previous loop of $\textbf{strategies}$, this loops around the agents in turn (can be $\textbf{row\_agents}$ or $\textbf{col\_agents}$) and contains an ``if'' statement, which conditions if  the strategy from the $\textbf{agent}$ in turn is equal to the counter in the ``main'' loop ($\textbf{strategies}$) the last value of the list $\textbf{frequency\_per\_strategy\_per\_agent}$ will increment by 1. When both loops finish the function returns a list with all the values that are contained in the $\textbf{frequency\_per\_strategy\_per\_agent}$, each of those values are devided by the number agents. This values are the proportion that strategy represents in the type of agents in turn. It is important to note that the variable $\textbf{agents}$ taken by this function can be either $\textbf{row\_agents}$ or $\textbf{col\_agents}$   and the variable $\textbf{strategies}$ can be either $\textbf{row\_strategies}$ or $\textbf{col\_strategies}$. 

When the method $\textbf{distributions}$ starts first it creates a variable $\textbf{row\_strategies\_distribution}$ this variable contains a function $\textbf{proportion\_classified\_strategies( )}$ that takes as attributes $\textbf{row\_agents}$ and $\textbf{row\_strategies}$. And then has a ``for'' loop which loops around the number of  $\textbf{row\_strategies}$ and appends into the list created at the initialization $\textbf{row\_accumulated\_strategies}$ the resulting value from the $\textbf{row\_strategies\_distribution}$. According to which strategy it belongs, it appends the value to the corresponding list. 
Then the method creates another variable which has the same process as the previous, but with $\textbf{col\_agents}$. The variable it creates is $\textbf{col\_strategies\_distribution}$ this variable contains a function $\textbf{proportion\_classified\_strategies( )}$ that takes as attributes $\textbf{col\_agents and col\_strategies}$. Then has a ``for'' loop which loops around the number of  $\textbf{col\_strategies}$ and appends into the list created at the initialization $\textbf{col\_accumulated\_strategies}$ the resulting value from the $\textbf{col\_strategies\_distribution}$. According to which strategy it belongs, it appends the value to the corresponding list.
After the two variables, and the append of $\textbf{row\_strategies\_distribution[  ]}$ to $\textbf{row\_accumulated\_strategies[  ]}$ and $\textbf{col\_strategies\_distribution[  ]}$ to $\textbf{col\_accumulated\_strategies[  ]}$ is finished. The method $\textbf{prints}$ for lines the first one contains the following text $\textit{``Row players' strategy distribution:"}$, the second line prints the list $\textbf{row\_strategies\_distribution}$, the third line prints the text $\textit{``Column players' strategy distribution:"}$, and the fourth line prints the list $\textbf{col\_strategies\_distribution}$. 
After these lines come two lines, each with the function $\textbf{kill\_agents( )}$ from the genetic algorithm module. One of which is for eliminating $\textbf{row\_agents}$ and the other for eliminating $\textbf{col\_agents}$.  
Then we have other two lines, each with the function $\textbf{reproduce\_agents( )}$ from the genetic algorithm module. One to create the $\textbf{row\_agents}$ that were previously eliminated until reaching the required number of $\textbf{row\_agents}$ and the other to create $\textbf{col\_agents}$ until reaching the required number of $\textbf{col\_agents}$. 
After the last $\textbf{reproduce\_agents( )}$ function, a code to update the graph is run (if it was chosen to graph at the beginning of the simulation). 
The whole cycle is repeated until the simulation reaches the number of $\textbf{generations}$ that was established at the beginning. 

\begin{figure}[H]
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [simblock, fill=green!80] (sim) {\textbf{Simulation} \\
	``Create a class to start the simulation with the following values, instances of the classes Agents, Genetic and 					BiMatrixRandomEnv are created. Also random initial strategies are assigned to the created agents (row and column agents)''};
    \node [simblock, below of=sim, node distance=5cm] (run) {\textbf{run} \\
	``This method is to start the simulation because it contains a function generations\_passing( ), and in here it is decided if there 		will be a graph representing the strategies or not''};
    \node [simblock, below of=run, node distance=3.5cm] (generations) {\textbf{generations\_passing} \\
	``This method keeps the count of the generations, which is the number of time the simulation will run''};
    \node [simblock, right of=generations, node distance=6.5cm] (distributions) {\textbf{distributions}\\
	``This method assigns the calculated distributions to the to the lists of row\_accumulated\_strategies and 						col\_accumulated\_strategies. And at the end it contains the functions to eliminate and create agents, and updates the 			graph''};
    \node [simblock, right of=distributions, node distance=6.5cm] (proportion) 											{\textbf{proportion\_}\textbf{classified\_} \\ 
	\textbf{strategies} \\
	``This method is used to convert the frequency of every strategy per type of agent (row or column agent) into proportions''};
    \node [simblock, below of=generations, node distance=3cm] (end) {\textbf{End} \\
	After all generations have passed, the final plot is shown and the simulations ends.};
    % Draw edges
    \path [line] (sim) -- node[left] {1}  (run);
    \path [line] (run) -- node[left] {2}  (generations);
    \draw [->, thick, dotted, black!100] (generations) edge [out=25, in=155] node[above] {3}  (distributions);
    \draw [->, thick, dotted, black!100] (distributions) edge [out=205, in=335] node[below] {6}  (generations);
    \draw [->, thick, dotted, black!100] (distributions) edge [out=30, in=150] node[above] {4}  (proportion);
    \draw [->, thick, dotted, black!100] (proportion) edge [out=210, in=330] node[below] {5}  (distributions);
    \path [line] (generations) -- node[left] {7} (end);
\end{tikzpicture}
\caption{Diagram of simulation module.}
\label{fig:diagsim}
\end{figure}


\newpage
\subsubsection{Overall interaction view}
The following diagram represents how the modules interact. Simulation contains the messages for all the other modules and with this it gives the sequence in which the modules work. 

\begin{itemize}
   \item $\textbf{1. }$ Create agents without a strategy, and with a utility equal to 0 according to the initial number of agents and the given initial distribution. If no initial distribution is entered it will create agents in equal proportions. 
   \item $\textbf{2. }$Then a random strategy is assigned according to the available strategies from the payoff matrix.
   \item $\textbf{3. }$Then the different agent strategies interact, and accumulate a utility. Each interaction is equivalent to a round, so the interactions are repeated according to the number of rounds established at the beginning of the simulation.
   \item $\textbf{4. }$The proportions of the agents present in the population are calculated to be plotted.
   \item $\textbf{5. }$Agents with low utilities are eliminated according to the death rate, since a constant size of population is desired  the same number of agents that were eliminated are created. The new agents are created according to the mutation rate and exploitation rate established at the beginning of the simulation. With low mutation rate and high exploitation rate the agents with highest utility are more likely to be reproduced, this means the new agents will have the same strategy as the agents with highest utility.
   \item $\textbf{6. }$Points 2, 3, 4 and 5 describe the sequence of the code for each generation. And will be repeated according to the number of generations established at the beginning of the simulation.
   \item $\textbf{7. }$After the iteration through the number of generations has finished, the final plot describing the behaviour of the strategies and a stackplot representing the proportions of the strategies in the populations are presented.
   \item $\textbf{8. }$The simulation ends.
\end{itemize}

\begin{figure}[H]
\begin{tikzpicture}
    \node [simblock] (simulation) at (0,0) [draw, fill=green!70] {\textbf{Simulation} \\
	``When creating an instance of Simulation, the simulation starts. Taking attributes type of game, number of agents, 				generations, rounds per generation, death rate, mutation rate, exploitation rate and its optional to choose an initial strategy 			distribution. And during the simulation, this module refers to other modules, keeps count of the generations that have passed 			and produces a final plot and stackplot where the strategies are shown.''};
    \node [simblock, below of=simulation, node distance=6.5cm](bimatrix)[draw, fill=gray!70] {\textbf{BiMatrixRandomEnv} \\
	``This creates agents without strategies and zero initial utility from the class Agent from the agents module. And during the 			simulation makes the agents interact, by pairing them and from the payoff matrix assigns the utility to each agent''}; 
    \node [simblock, right of=bimatrix, node distance=6cm] (agents)[draw, fill=blue!70] {\textbf{Agent} \\
	``This module will be used by BiMatrixRandomEnv class from environment module to create agents''};
    \node [simblock, below of=bimatrix, node distance=6.5cm](genetic) [draw, fill=purple!70] {\textbf{Genetic} \\ 
	``This class from module genetic at the beginning of the simulation will assign the strategies to the different agents by default 		or according to an initial distribution that will be set in the Simulation instance. During the simulation it will eliminate agents with 			the lowest accumulated utility for each generation and will create new agents according to the initial setting (mutation rate and 	exploitation rate.''};
    \node [geneblock, left of=bimatrix, node distance=7.7cm](generations) [draw] {\textbf{generations}\\ 
	``Is only equivalent to the number of times the simulation will be repeated, and the count will be kept by the method 				generations\_passing from the instance Simualtion.''};   
    \node [geneblock, right of=simulation, node distance=6cm](end) [draw] {\textbf{End of the simulation}\\ 
	``This does not exist in the module, but it represents the ending of the simulation after having iterated through all the 				generations, and at this stage the final plot is presented as well as the stackplot if it was required at the beginning when 			running the simulation.''};   

    % Draw edges 
    \path [line] (simulation) --  node[left] {1} (bimatrix);
    \path [line] (bimatrix) -- node[left] {2}  (genetic);
    \path [line, dashed] (agents) --  node[above] {(1)}  (bimatrix);
    \draw [->, thick, dotted, black!100] (genetic) edge [out=30, in=330] node[left] {3}  (bimatrix);
   \draw [->, thick, dotted, black!100] (bimatrix) edge [out=30, in=330] node[left] {4}  (simulation);
    \draw [->, thick, dotted, black!100] (simulation) edge [out=210, in=150] node[right] {5}  (genetic);
    \path [line] (genetic) -| node[left] {6}  (generations);
    \path [line] (generations) |-  node[left] {6}  (simulation);
    \path [line, dashed] (simulation) --  node[above] {7}  (end);
\end{tikzpicture}
\caption{Diagram interaction of modules.}
\label{fig:diagoverall}
\end{figure}



\newpage
\section{Application of the code}


In this section the runs of the program will be discussed. What was consider when running a simulation. An explanation of what games were simulated, and of the results against the known techniques to indicate how the program outputs our expected results when running it with known classic game theory games. A comparison of the results given by the  library 'Axelrod' from python with results from this code 'Ablearn'.

\subsection{How the simulation was built.}
After building the code the objective is to determine if the program gives us the expected results. We know that an evolutionary stable strategy is a strategy that survives through time. And as we have mentioned before there are certain conditions that we can verify in order to identify and evolutionary stable strategy. The following tables represent some of the most well known examples of games in the normal form. 

As said before, when using the code for simulating there are different variables we can input which will have an impact in the resulting output. The input rates that have a greater impact are death rate, which determines how many agents will be eliminated per generation; mutation rate, which will make the most efficient strategy in accumulating utility in each generation reproduce but with different characteristics (strategy); exploitation rate, which can be modified so that only the agent with highest utility will reproduce or to take a percentage of the population to be considered for reproducing.

Perhaps the most known is the the prisoner's dilemma which was mentioned before and it basically has two Nash equilibria strategies one in which both cooperate and the other in which non cooperate.

\begin{table}[h]
\begin{center}
Prisoner 2

Prisoner1
\begin{tabular}{|l|c|c|}
\hline
 & Cooperate & Defect \\ 
\hline
Cooperate & 3, 3 & 0, 5\\
\hline
 Defect & 5, 0 & 1, 1\\
\hline
\end{tabular}
\caption{Prisoner's Dilemma}
\label{tab:prisdiltag}
\end{center}
\end{table}


Matching pennies which is a pure conflict zero-sum game in which the winner of the game takes all and the loser ends up losing her share. We see that there is no scenario where both players can agree in a strategy. Player 1 preference of choice is from tossing a coin involves getting 2 heads or 2 tails, whilst player 2 prefers having a mixed combination.  The equilibrium for this game is a mixed Nash equilibrium.

\begin{table}[h]
\begin{center}
Player 2


Player 1
\begin{tabular}{|l|c|c|}
\hline
 & Heads & Tails \\ 
\hline
Heads & 1, -1 & -1, 1\\
\hline
 Tails & -1, 1 & 1, -1\\
\hline
\end{tabular}
\end{center}
\caption{Matching Pennies}
\label{tab:matpentag}
\end{table}


The battle of sexes game is a coordination game in which both agents cannot exchange information about what option out of two to choose, they both have a preferred strategy, but if they both choose their preferred strategy they have no payoff from it because they rather choose the same strategy and concur,  even if it has a higher payoff for one of them than the other. In the example from the table there is a representation of preferences, and we should assume that it is a couple trying to decide where to go, the female agent prefers going to the opera, whilst the male agent prefers going to watch football. But we see that if they both end up choosing different strategies from each other they get a payoff of 0. 

\begin{table}[h]
\begin{center}
Female


Male
\begin{tabular}{|l|c|c|}
\hline
 & Opera & Football \\ 
\hline
Opera & 1, 2 & 0, 0\\
\hline
 Football & 0, 0 & 2, 1\\
\hline
\end{tabular}
\end{center}
\caption{Battle of Sexes}
\label{tab:bostag}
\end{table}

And the hawk-dove game. This game is often used in  evolutionary game theory. And it represents the 2 strategies an agent can choose,  and the result of the interaction. There is an aggressive strategy which is the hawk strategy and a passive strategy which is the dove. When both agents choose to play hawk, the payoff they get is 0, the explanation is that since they are both aggressive, the possible payoff they could have had from the resource they are competing for is not greater than the cost they pay for playing this strategy against each other. When they both choose dove, they split in equal parts the resource and the payoff they receive is the same for both. When one plays hawk and the other dove, the agent using hawk strategy gets a higher payoff than the one using the dove strategy.    

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
 & Hawk & Dove \\ 
\hline
Hawk & 0, 0 & 3, 1\\
\hline
 Dovel & 1, 3 & 2, 2\\
\hline
\end{tabular}
\end{center}
\caption{Hawk-Dove}
\label{tab:hdtag}
\end{table}

\newpage
\section{Results}
\subsection{Prisoner's Dilemma}
The following table indicates how the simulations for prisoner's dilemma were run.
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Num. & Population & Generations& Rounds per Gen. & Death Rate & Mutation Rate & Exploitation Rate\\ 
\hline
1& 1000 & 50 & 50 & 0.1 & 0.1 & 1\\
\hline
2& 1000 & 50 & 25 & 0.1 & 0.1 & 1\\
\hline
3& 1000 & 50 & 5 & 0.1 & 0.1 & 1\\
\hline
4& 1000 & 50 & 5 & 0.5 & 0.1 & 1\\
\hline
5& 1000 & 50 & 5 & 0.9 & 0.1 & 1\\
\hline
6& 1000 & 50 & 5 & 0.9 & 0.5 & 1\\
\hline
7& 1000 & 50 & 5 & 0.9 & 0.9 & 1\\
\hline
8& 1000 & 50 & 5 & 0.1 & 0.5 & 1\\
\hline
9& 1000 & 50 & 5 & 0.1 & 0.9 & 1\\
\hline
10& 1000 & 50 & 5 & 0.1 & 0.1 & 0.5\\
\hline
11& 1000 & 50 & 5 & 0.5 & 0.1 & 0.5\\
\hline
12& 1000 & 50 & 5 & 0.9 & 0.1 & 0.5\\
\hline
13& 1000 & 50 & 5 & 0.1 & 0.5 & 0.5\\
\hline
14& 1000 & 50 & 5 & 0.5 & 0.5 & 0.5\\
\hline
15& 1000 & 50 & 5 & 0.9 & 0.5 & 0.5\\
\hline
16& 1000 & 50 & 5 & 0.1 & 0.9 & 0.5\\
\hline
17& 1000 & 50 & 5 & 0.5 & 0.9 & 0.5\\
\hline
18& 1000 & 50 & 5 & 0.9 & 0.9 & 0.5\\
\hline
19& 1000 & 50 & 5 & 0.1 & 0.1 & 0.1\\
\hline
20& 1000 & 50 & 5 & 0.5 & 0.1 & 0.1\\
\hline
21& 1000 & 50 & 5 & 0.9 & 0.1 & 0.1\\
\hline
22& 1000 & 50 & 5 & 0.1 & 0.5 & 0.1\\
\hline
23& 1000 & 50 & 5 & 0.5 & 0.5 & 0.1\\
\hline
24& 1000 & 50 & 5 & 0.9 & 0.5 & 0.1\\
\hline
25& 1000 & 50 & 5 & 0.1 & 0.9& 0.1\\
\hline
26& 1000 & 50 & 5 & 0.5 & 0.9 & 0.1\\
\hline
27& 1000 & 50 & 5 & 0.9 & 0.9 & 0.1\\
\hline
28& 1000 & 50 & 5 & 0.5 & 0.5 & 1\\
\hline
29& 1000 & 50 & 5 & 0.5 & 0.9 & 1\\
\hline

\end{tabular}
\end{center}
\caption{Configuration of simulations for prisoner's dilemma.}
\label{tab:simpdtag}
\end{table}

\newpage

For Prisoner's dilemma I do not consider relevant to vary the number of generations to test for any changes. This is because the interaction of the agents and their strategies is given in the level of rounds. In other words, the rounds give the proportion of strategies for each generation. A variation in the number of generations may be considered when a clear pattern in the data is not easy to identify. However, I do consider important to determine if the number of rounds make a difference in the results. Hence the simulation will be tested  with different number of rounds. It will be done under the conditions 1, 2 and 3 of the table 4.5 (Configuration of simulations).
\\\\The simulations start by assigning the following proportions to row and column agents to the strategies cooperate and defect

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.49 & 0.51\\
\hline
Column Agents & 0.502 & 0.498\\
\hline
\end{tabular}
\end{center}
\caption{ Initial Prisoner’s Dilemma distribution for simulation 1.}
\label{tab:pds1g1}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.489 & 0.514\\
\hline
Column Agents & 0.524 & 0.476\\
\hline
\end{tabular}
\end{center}
\caption{Initial Prisoner’s Dilemma distribution for simulation 2.}
\label{tab:pds2g1}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.512 & 0.488\\
\hline
Column Agents & 0.49 & 0.51\\
\hline
\end{tabular}
\end{center}
\caption{ Initial Prisoner’s Dilemma distribution for simulation 3.}
\label{tab:pds3g1}
\end{table}

 The 3 simulations start with very similar proportions for each strategy for agent type. They all start close to 0.5 so there is not a significant difference at this stage.

By generation 5 it can be seen that the strategy `Defect' is dominating.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.348 & 0.652\\
\hline
Column Agents & 0.24 & 0.76\\
\hline
\end{tabular}
\end{center}
\caption{ Generation 5 Prisoner’s Dilemma distribution for simulation 1.}
\label{tab:pds1g5}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.25 & 0.75\\
\hline
Column Agents & 0.378 & 0.622\\
\hline
\end{tabular}
\end{center}
\caption{Generation 5 Prisoner’s Dilemma distribution for simulation 2.}
\label{tab:pds2g5}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.35 & 0.65\\
\hline
Column Agents & 0.314 & 0.686\\
\hline
\end{tabular}
\end{center}
\caption{Generation 5 Prisoner’s Dilemma distribution for simulation 3.}
\label{tab:pds3g5}
\end{table}
For simulation 2 in generation 5 it can be seen that the proportions are slightly different than first simulation. For row agents the proportion increased and is very close to the proportion of the column agents in generation 5 from the previous simulation 1 and for the column agents decreased and is very close to the proportion for row agents in the simulation 1.
In simulation 3 for this generation the value for strategy `Defect' for row agents is slightly higher than simulation 1 and slightly smaller than simulation 2. And for the same strategy in column agents compared to simulation 1 it decreased slightly and to simulation 2 it increased. The differences presented between simulations do not appear significant for this generation.

By generation 10 it can be seen that the dominance of strategy `Defect'  is almost absolute in the 3 simulations.
\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.054 & 0.946\\
\hline
Column Agents & 0.074 & 0.926\\
\hline
\end{tabular}
\end{center}
\caption{Generation 10 Prisoner’s Dilemma distribution for simulation 1.}
\label{tab:pds1g10}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.018 & 0.982\\
\hline
Column Agents & 0.094 & 0.906\\
\hline
\end{tabular}
\end{center}
\caption{Generation 10 Prisoner’s Dilemma distribution for simulation 2.}
\label{tab:pds2g10}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.082 & 0.918\\
\hline
Column Agents & 0.07 & 0.93\\
\hline
\end{tabular}
\end{center}
\caption{Generation 10 Prisoner’s Dilemma distribution for simulation 3.}
\label{tab:pds3g10}
\end{table}

In simulation 2 for generation 10  with respect to simulation 1, a small increase in the strategy `Defect'  for row agents, and a small decrease in the same strategy for column agents. For simulation 3 strategy `Defect' for row agents is slightly decreased with respect to the value in simulation 1 and 2. For the same strategy in column agents it increased slightly when compared to simulation 1 and 2.
In this generation for the 3 different simulations, the value for strategy `Defect' are above 0.9, but again there is no obvious pattern so the variations may be given by the effect of the randomized variables for each simulation.

In generation 25 it can be seen that the condition persists and the strategy `Defect' for both types of agents dominates in frequency.
\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.068 & 0.932\\
\hline
Column Agents & 0.054 & 0.946\\
\hline
\end{tabular}
\end{center}
\caption{Generation 25 Prisoner’s Dilemma distribution for simulation 1.}
\label{tab:pds1g25}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.045 & 0.955\\
\hline
Column Agents & 0.047 & 0.953\\
\hline
\end{tabular}
\end{center}
\caption{Generation 25 Prisoner’s Dilemma distribution for simulation 2.}
\label{tab:pds2g25}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.072 & 0.928\\
\hline
Column Agents & 0.054 & 0.972\\
\hline
\end{tabular}
\end{center}
\caption{Generation 25 Prisoner’s Dilemma distribution for this simulation 3.}
\label{tab:pds3g25}
\end{table}


In simulation 2, the proportions increased slightly for strategy defect for both agents in relation to simulation 1. For simulation 3 the proportions are slightly smaller for the strategy `Defect' for row agents than in simulations 1 and 2. For same strategy in column agents is slightly higher than values from simulations 1 and 2. Still all the values for strategy `Defect' are above 0.9.

For generation 50, the last generation we see that strategy `Defect' dominates the strategy `Cooperate' for all simulations.The graphs from the 3 different simulations are presented.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.01 & 0.99\\
\hline
Column Agents & 0.044 & 0.956\\
\hline
\end{tabular}
\end{center}
\caption{ Generation 50 Prisoner’s Dilemma distribution for simulation 1.}
\label{tab:pds1g50}
\end{table}



\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{1pd50}


\caption{ Prisoner's dilemma 50 rounds per generation 1.}
\label{fig:pds1}
\end{center}
\end{figure}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.024 & 0.976\\
\hline
Column Agents & 0.08 & 0.92\\
\hline
\end{tabular}
\end{center}
\caption{Generation 50 Prisoner’s Dilemma distribution for simulation 2.}
\label{tab:pds2g50}
 \end{table}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{2pd25}

\caption{ Prisoner's Dilemma 25 rounds per generation.}
\label{fig:pds2}
\end{center}
\end{figure}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
& Cooperate & Defect \\ 
\hline
Row Agents & 0.02 & 0.98\\
\hline
Column Agents & 0.028 & 0.972\\
\hline
\end{tabular}
\end{center}
\caption{Generation 50 Prisoner’s Dilemma distribution for simulation 3.}
\label{tab:pds3g50}
 \end{table}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{3pd5}

\caption{Prisoner's Dilemma 5 rounds per generation.}
\label{fig:pds3}
\end{center}
\end{figure}

Simulation 2, both proportions decreased slightly. Since the changes in the proportions are very small I do not consider there is significant difference between using 50 or 25 rounds. For simulation 3 the value for row agent in strategy `Defect' is slightly higher compared to simulation 2  and smaller from the one in simulation 1. For column agents for the same strategy the value is higher in comparison to the values from simulations 1 and 2.


After the 3 runs for the simulations, we can see in the graphs from each simulation that they all have a similar pattern. And after analysing the different specific points (generations), the variations between them do not appear to be caused by the number of rounds. The strategies through the generation have a similar behaviour, the strategy `Defect' for row and column agents dominate the strategy `Cooperate'. The strategy `Cooperate' in the 3 different simulations starts declining from as early as the second generation, and around generation 10 it reaches the lowest values continuing to oscillate between 0.02 and 0.06 for the rest of the simulation. Again it is important to mention that the length of the simulations in terms of generations do not appear to be very relevant, since we can see that the behaviour for both strategies is very similar from generation 10 to generation 50 (when the simulation ends). On the other hand, we can see that the behaviour for the strategies is cyclical, with not a very clear pattern. For this reason in prisoner's dilemma simulations the configuration with 5 rounds per generation will used.
When there is a small increment in the `Cooperate' or `Defect' is given to the mutation rate, that is allowing more of either to appear, but since it is very low (0.1) it does not change the behaviour significantly. This is how the graphs behave without mutation rate.

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{1pdno}

\caption{ Prisoner's Dilemma 50 rounds per generation mutation rate 0.}
\label{fig:pds1mr0}	
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{2pdno}

\caption{ Prisoner's Dilemma 25 rounds per generation mutation rate 0.}
\label{fig:pds2mr0}		
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{3pdno}

\caption{ Prisoner's Dilemma 5 rounds per generation mutation rate 0.}
\label{fig:pds3mr0}	
\end{center}
\end{figure}


These graphs show how mutation rate allows the less effective strategy `Cooperate' to be present throughout the generations, and without mutation they will just disappear as early as the 5th generation. 
Let us continue to try the different combinations of values that are shown in the table 4.5 (Configuration of simulations). 
\\\\For simulations 3, 4 and 5 we have constant values for mutation rate with 0.1 and exploitation rate of 1. The resulting graphs from simulations 3, 4, and 5 are presented. It is important to note that in these first 3 simulations the value that varies is death rate, and that the smaller the death rate, the gap between strategies `Defect' and `Cooperate' is larger.

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{3pd5}
	
\caption{Prisoner's Dilemma simulation 3.}
\label{fig:pds3}	
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{4pd5}
	
\caption{Prisoner's Dilemma simulation 4.}
\label{fig:pds4}	
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{5pd5}

\caption{ Prisoner's Dilemma simulation 5.}
\label{fig:pds5}	
\end{center}
\end{figure}


Graphs 5, 6 and 7  have a high death rate of 0.9, this means that it allows the creation off 90\% of new agents each generation while eliminating the least efficient. These three graphs have mutation rates of 0.1, 0.5 and 0.9 respectively with an exploitation rate of 1. And the behaviour can be seen in the following graphs.

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{5pd5}
	
\caption{ Prisoner's Dilemma simulation 5}
\label{fig:pds5}	
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{6pd5}
	
\caption{Prisoner's Dilemma simulation 6.}
\label{fig:pds6}	
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{7pd5}

\caption{ Prisoner's Dilemma simulation 7.}
\label{fig:pds7}	
\end{center}
\end{figure}

It can be seen that when the mutation rate is lower the variation is higher, even if the proportions are close to 0.5. There are evident high peaks and low valleys  for both strategies. In the three simulations the strategy `Defect' dominates in general but in simulation 5  for generations 5, 7, 17, 23, 27, 34, 36 and 48, the strategy `Cooperate' briefly dominates `Defect' in fact it only dominates in this generations in the previous of next generation the values are lower. This brief dominance from `Cooperate' may be given to a higher interaction between this type of strategy between each other, in consequence they accumulate a higher utility which allows them to reproduce in the following generation. Because of the high death rate they reproduce quickly. But this means that in the following generation there is a higher probability for `Defect' to encounter with `Cooperate', so `Defect' accumulates more utility and  `Cooperate' quickly dies out. In the other 2 simulations we see how as the mutation level increases the proportion of the strategies become more stable. By looking at the graphs 1, 8 and 9 have a similar behaviour where as the mutation rate increases, less peaks and valleys can be seen.

For graphs 10, 11 and 12 we have an incrementing death rate of 0.1, 0.5 and 0.9 respectively. For all 3 simulations there are constant values for mutation rate of 0.1 and exploitation rate of 0.5. As the death rate increases, as expected the gap between the number of strategies representing `Defect' and `Cooperate' reduces. But it can also be seen that as the as death rate increases given the value of 0.5 for exploitation rate the peaks and valleys increase, they do not get to the point where `Cooperate' dominates `Defect', but it is important to note that with a low mutation rate and a lower exploitation rate than 1 which in these cases is 0.5 strategies that earn lower utility during the interaction are allowed to reproduce in the same probability as the one with highest utility. 

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{10pd5}

\caption{ Prisoner's Dilemma simulation 10.}
\label{fig:pds10}	
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{11pd5}
	
\caption{ Prisoner's Dilemma simulation 11.}
\label{fig:pds11}	
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{12pd5}

\caption{ Prisoner's Dilemma simulation 12.}
\label{fig:pds12}	
\end{center}
\end{figure}

When comparing these graphs with the graphs from simulation 3, 4 and 5 which have an increasing value of death rate and a death rate of 0.1 they behave in a similar way. But when comparing graphs from simulation 5 and 12 with each other. They both have a death rate of 0.9 and a mutation rate of 0.1, but a different exploitation rate value of 1 and 0.5 respectively. We can see that since a lower exploitation rate makes other strategies reproduce with the same probability as the strategy with the highest accumulated utility from the generation, a larger gap between strategies is present. Meaning that even if `Cooperate' accumulates a high utility, if there is `Defect' strategy within the 50\% of the population that is allowed to reproduce, it has a possibility to reproduce. This `randomization' gives an equilibrium between the presence of strategies in the population. Simulations 19, 20 and 21 have the same values as these other groups of simulations, but an exploitation rate of 0.1, which makes makes graph from simulation 21 behave very similar to the graph from simulation 12.

For the graphs from simulations 16, 17 and 18 the value that changes is death rate 0.1, 0.5 and 0.9 respectively. With constant values of mutation rate with 0.9 and exploitation rate 0.5. As we have seen before the effect of the death rate reflects with a reducing gap between the type of strategies as death rate increases. 
 

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{16pd5}
	
\caption{ Prisoner's Dilemma simulation 16.}
\label{fig:pds16}	
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{17pd5}
	
\caption{ Prisoner's Dilemma simulation 17.}
\label{fig:pds17}	
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{18pd5}

\caption{ Prisoner's Dilemma simulation 18.}
\label{fig:pds18}	
\end{center}
\end{figure}

But we can see a particular characteristic in the graph for simulation 16, it reaches the lowest value for `Cooperate' strategy later than other simulations with the same death rate. Graphs from simulations 3, 8, 9, 10, 13, 19 and 22 they all reach the lowest value for `Cooperate' around generation 10. Simulation 16 reaches it closer to generation 20, we see a similar behaviour for simulation 25. What this two simulations have in common other than death rate value, is their mutation rate is 0.9. Since mutation rate allows any strategy to reproduce except for the agent with strategy with that accumulated the highest utility, this delays the `Cooperate' strategy for reaching its lowest value.		  
The remaining simulations, behave in a similar way to the one that have been discussed  so far. It can be seen that the variable that acts as an enabler for the other variables to have an effect is the death rate. 

\subsubsection{Nash Equilibrium}
As we calculated before in this work. We can find the Nash Equilibrium for the prisoner's dilemma game, by looking at the normal form table and select the best responses for each player. We can see that the results from the simulations gives us the strategy ``Defect'' as stable and this is also a Nash equilibrium as we can see in the normal form table for the game.

\begin{table}[H]
\begin{center}
Player 2

Player 1
\begin{tabular}{|l|c|c|}
\hline
 & Cooperate & Defect\\ 
\hline
Cooperate & 3, 3 & 0, 5\\
\hline
Defect & 5, 0 & \underline{1}, \underline{1}\\
\hline
\end{tabular}

\caption{ Prisoner's dilemma Nash equilibrium with best responses.}
\label{fig:pdnashbr}	
\end{center}
\end{table}


\subsubsection{ESS}
Now let us find the ESS for the prisoner's dilemma game. Earlier it was explained how to calculate the evolutionarily stable strategy (ESS) when a mutant strategy appears in the population in the proportion $\epsilon$. We will assume the strategy  `Cooperate' is the population and the invader is the strategy `Defect'. Thus we have that the payoff for the strategy `Cooperate' is the following:
\begin{equation}
(1-{\epsilon})3 + 0{\epsilon} = 3 - 3{\epsilon}
\end{equation}
And the payoff for the strategy `Defect' is:
\begin{equation}
(1-{\epsilon})5 + 1{\epsilon} = 5 - 4{\epsilon}
\end{equation}
We can see that the strategy `Defect' appears to be greater than `Cooperate'. Now since our simulation every time gives almost equal starting proportions to each strategy we will assume that the `invading' strategy `Defect' appears in a proportion of $\epsilon$=0.5. And we have the following:
\begin{equation}
3 - 3(0.5) = 1.5
\end{equation}
And for the strategy `Defect' is:
\begin{equation}
 5 - 4(0.5) = 3
\end{equation}
So we have that the payoff for `Defect' is greater than the payoff for `Cooperate' thus we can say that `Cooperate' is not an evolutionarily stable strategy.
Now we check if the strategy `Defect' is evolutionarily stable. And therefore we assume that `Cooperate is the invader. We can see that it appears that `Defect' might be evolutionarily stable given the previous test. But we should check this in the same way. We first have the expected payoff of the strategy `Defect':
\begin{equation}
(1-{\epsilon})1 + 5{\epsilon} = 1 + 4{\epsilon}
\end{equation}
And the payoff for the strategy `Defect' is:
\begin{equation}
(1-{\epsilon})0 + 3{\epsilon} =  3{\epsilon}
\end{equation}
Again it appears that the strategy `Defect' has a greater expected payoff than `Cooperate'. We assume that the `invading' strategy `Cooperate' appears in a proportion of $\epsilon$=0.5. And we have the following for the strategy `Defect':
\begin{equation}
1 +  4(0.5) = 3
\end{equation}
And for the strategy `Cooperate' is:
\begin{equation}
 3(0.5) = 1.5
\end{equation}
So we see that again the strategy`Defect' has a higher expected payoff, so it is an evolutionarily stable strategy (ESS).
With this we can see that the strategy `Defect' is both a Nash equilibrium and an ESS. And this supports the results we have from the simulation.


\subsection{``Matching Pennies''}

For the simulations in matching pennies, generations with 5, 25 and 100 rounds per generation were made.  The table for the configurations is similar to the one used in Prisoner's dilemma simulations, and it can be found in APPENDIX???..


For simulations 1, 2 and 3 with 5, 25 and 100 rounds per generation respectively. It can be seen that as the rounds per generation increase the amplitude of the wave grows. This means that with more rounds per generations each strategy has a greater chance of reaching a higher number in population, by reproducing when accumulating a high utility. This is also given to the low death rate, the low mutation rate and high exploitation rate which allows the most efficient strategy in the generation to reproduce almost every change of generation. But also when a population of a certain strategy increases in this kind of game, it is likely that a strategy that is  earns high payoff when interacting with the strategy with high population will grow in population too given that it is more likely to encounter its counterpart and this cycle repeats with all the available strategies and we can see a cycle. 

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{1mp100}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{2mp25}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{3mp5}}
    \end{center}
    \caption{Simulation 1, 2 and 3}
    \label{firstthreesimulations}
\end{figure}

The behaviour for the graphs for each simulation is similar with an increasing amplitude until simulation 10 for the three different number of rounds. For simulation 10 the amplitude increases when having a lower exploitation rate in combination with low death rate (0.1) and low mutation rate (0.1) in a very similar way for the different number of rounds. This behaviour is because the exploitation rate allows the 50\% of the population to reproduce with the same probability. So almost any strategy can reproduce unless it has the lowest accumulated utility, which means they will be eliminated. And the low death rate allows them to accumulate in a greater number.

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{10mp5}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{10mp25}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{10mp100}}
    \end{center}
    \caption{Simulation 10 for 5, 25 and 100 rounds per generation respectively}
    \label{simulation10mp}
\end{figure}

In simulation 12  the death rate of 0.9 increases the frequency in oscillations when combined with the 0.5 exploitation rate, all with a very similar amplitude. Which means that the population of certain strategy increases fast and is eliminated fast given the high death rate, and the broad possibilities of different agents reproducing with the exploitation rate of 0.5.

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{12mp5}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{12mp25}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{12mp100}}
    \end{center}
    \caption{Simulation 12 for 5, 25 and 100 rounds per generation respectively}
    \label{simulation12mp}
\end{figure}

In simulation 13, 14 and 15 we can see an interesting behaviour when an increasing death rate of 0.1, 0.5 and 0.9 respectively is combined with the constant values of 0.5 for mutation rate and 0.5 for exploitation rate.


\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{13mp5}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{14mp5}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{15mp5}}
    \end{center}
    \caption{Simulation 13, 14 and 15 for 5 rounds per generation respectively}
    \label{simulation131415mp5}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{13mp25}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{14mp25}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{15mp25}}
    \end{center}
    \caption{Simulation 13, 14 and 15 for 25 rounds per generation respectively}
    \label{simulation131415mp25}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{13mp100}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{14mp100}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{15mp100}}
    \end{center}
    \caption{Simulation 13, 14 and 15 for 100 rounds per generation respectively}
    \label{simulation131415mp100}
\end{figure}
It can be seen an increasing amplitude with the different increasing number of rounds, but in simulation 15 with death rate of 0.9 the amplitude for all of them decreases. This is given to the mutation rate of 0.5 in combination with 0.5 exploitation rate, these values make the creation of new agents with a high possibility of randomness, so the agent with the highest accumulated utility has same less probability of reproducing than others because the mutation rate gives opportunity to any agent strategy except for that with the highest accumulated payoff. And we expect to see a similar behaviour when death rate is 0.9 with mutation rate of 0.9, which will practically rule out the possibility of the best agent strategy with the highest accumulated utility to reproduce.

In simulation 16, 17 and 18 we have constant values of mutation rate for 0.9 and exploitation rate of 0.5 with increasing death rate of 0.1, 0.5 and 0.9 respectively. For 16 and 17 when increasing death rate we see when comparing with 5, 25 and 100 rounds per generation, the amplitude increases. Something interesting with the high value of 0.9 in death rate that they virtually have the same amplitude.

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{16mp5}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{17mp5}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{18mp5}}
    \end{center}
    \caption{Simulation 16, 17 and 18 for 5 rounds per generation respectively}
    \label{simulation161718mp5}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{16mp25}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{17mp25}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{18mp25}}
    \end{center}
    \caption{Simulation 16, 17 and 18 for 25 rounds per generation respectively}
    \label{simulation161718mp25}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{16mp100}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{17mp100}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{18mp100}}
    \end{center}
    \caption{Simulation 16, 17 and 18 for 100 rounds per generation respectively}
    \label{simulation161718mp100}
\end{figure}
As expected with a high mutation rate (0.9) and high death rate (0.9) the amplitude is smaller, as seen in graphs from simulation 15 and in graphs from simulation 18.

For simulations 19, 20 and 21 we have a low constant value of 0.1 mutation rate and 0.1 exploitation rate, and increasing death rate 0.1, 0.5 and 0.9. The amplitude behaves the same with 5, 25 and 100 rounds, and the frequency increases for all the cases with the increasing death rate.

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{19mp5}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{20mp5}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{21mp5}}
    \end{center}
    \caption{Simulation 19, 20 and 21 for 5 rounds per generation respectively}
    \label{simulation192021mp5}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{19mp25}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{20mp25}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{21mp25}}
    \end{center}
    \caption{Simulation 19, 20 and 21 for 25 rounds per generation respectively}
    \label{simulation192021mp25}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{19mp100}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{20mp100}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{21mp100}}
    \end{center}
    \caption{Simulation 19, 20 and 21 for 100 rounds per generation respectively}
    \label{simulation192021mp100}
\end{figure}
This behaviour is given to the fact that the mutation rate is low, and the low exploitation (0.1) rate allows any  strategy to reproduce with the same probability including the agent strategy with the highest accumulated utility, and it makes the selection is not as random than when also having a high mutation rate, but still many agents are candidates to reproduce. So the high frequency in the graphs with high death rate is present because  a high number of agents are allowed to reproduce each generation and also the same number are eliminated. So the brief dominance of an agent strategy is given to the random factor. 
\\\\ For simulations 22, 23, 24 have an increasing death rate of 0.1, 0.5 and 0.9 respectively, with constant mutation rate of 0.5 and exploitation rate of 0.1. With death rate 0.1 we see an expected increase in the amplitude when increasing number of rounds. With death rate of 0.5 we see an increase in the frequency and a reduced amplitude for the graphs for 5, 25 and 100 rounds. And with death rate 0.9 a much smaller amplitude is perceived and still with a high frequency.

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{22mp5}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{23mp5}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{24mp5}}
    \end{center}
    \caption{Simulation 22, 23 and 24 for 5 rounds per generation respectively}
    \label{simulation222324mp5}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{22mp25}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{23mp25}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{24mp25}}
    \end{center}
    \caption{Simulation 22, 23 and 24 for 25 rounds per generation respectively}
    \label{simulation222324mp25}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{22mp100}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{23mp100}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{24mp100}}
    \end{center}
    \caption{Simulation 22, 23 and 24 for 100 rounds per generation respectively}
    \label{simulation222324mp100}
\end{figure}

As expected the only difference between simulation with 5, 25 and 100 rounds is only an increasing amplitude when increasing number of rounds. Also the expected reduced amplitude when death rate is increased since there is a high mutation rate and a relatively low exploitation rate, population is renewed almost disregarding the performance of each agent strategy.

With increasing death rate of 0.1, 0.5 and 0.9 for simulation 25, 26 and 27 respectively; and constant mutation rate of 0.9 and exploitation rate of 0.1 for the three simulations. We can see a similar behaviour are the previous 3 simulations analised.
\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{25mp5}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{26mp5}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{27mp5}}
    \end{center}
    \caption{Simulation 25, 26 and 27 for 5 rounds per generation respectively}
    \label{simulation252627mp5}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{25mp25}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{26mp25}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{27mp25}}
    \end{center}
    \caption{Simulation 25, 26 and 27 for 25 rounds per generation respectively}
    \label{simulation252627mp25}
\end{figure}

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{25mp100}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{26mp100}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{27mp100}}
    \end{center}
    \caption{Simulation 25, 26 and 27 for 100 rounds per generation respectively}
    \label{simulation252627mp100}
\end{figure}
Increasing amplitude as the number of rounds per generation are increased 5, 25 and 100. Although for these simulations incrementing amplitude is smaller because of the random factor that the mutation rate introduces (and how it excludes the agent strategy with highest accumulated utility). We can also see the incrementing frequency when using 0.5 and 0.9 death rate. And for simulation 27, we see that for the different number of rounds the amplitude and frequency are similar for all, again these  can be attributed to the high mutation rate and high death rate, but it is also important to mention that the low exploitation rate in combination with high mutation rate will have an effect on the simulation and we will find that all the agent strategies will be present in very similar proportions in the population. To have a better visual understanding we can see the following stackplot.  

\begin{figure}[H]       
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{mpstacksim27r5}}   
    \hspace{0px}
    \mbox{\includegraphics[scale=0.35]{mpstacksim27r25}}
    \hspace{40px}
    \end{center}
    \begin{center}
    \mbox{\includegraphics[scale=0.35]{mpstacksim27r100}}
    \end{center}
    \caption{Stackplots for simulation 27 with 5, 25 and 100 rounds per generation respectively}
    \label{simulation27mp525100}

\end{figure}

Strategy 1 is `Heads' and strategy 2 is `Tails'. We can see that each strategy during the whole simulation is present in the total population in a rate of  very close to 0.5. 

\subsubsection{Nash Equilibrium}
We will find the Nash Equilibrium for the matching pennies game. We cannot calculate it just by looking at the normal form table and selecting the best responses since this game has a mixed Nash equilibrium.

\begin{table}[H]
\begin{center}
Player 2

Player 1
\begin{tabular}{|l|c|c|}
\hline
 & Cooperate & Defect\\ 
\hline
Cooperate & 1, -1 & -1, 1\\
\hline
Defect & -1, 1 & 1, -1\\
\hline
\end{tabular}

\caption{ Matching pennies bimatrix payoff.}
\label{fig:mpnashmx}	
\end{center}
\end{table}

So we calculate the mixed Nash equilibrium by making equal the expected payoff  when playing `heads' to the expected payoff of playing `tails'. We will assume that the probability for player 1 of playing `heads' is $\sigma$ and for playing tails 1 - $\sigma$, resulting in the following:
\begin{equation}
(-1)(\sigma) + (1 - \sigma) = (1)(\sigma) + (-1)(1 - \sigma)
\end{equation}
\begin{equation}
-4 \sigma = -2
\end{equation}
\begin{equation}
\sigma = 1/2
\end{equation}
This means that player 1 will play `heads' with a probability of 1/2 , and also means she will be playing tails with 1/2. And now we calculate the probability of playing 'heads; for player 2 with probability $\sigma$ and `tails' with probability 1 - $\sigma$.

\begin{equation}
(1)(\sigma) + (1 - \sigma)(-1) = (-1)(\sigma) + (1)(1 - \sigma)
\end{equation}
\begin{equation}
4 \sigma = 2
\end{equation}
\begin{equation}
\sigma = 1/2
\end{equation}

This means that player 2 when playing against player 1 will play` heads'  1/2  of the times and in consequence play `tails' in the other 1/2 of the times. 
This means that the Nash equilibria is when:
\begin{equation}
((1/2, 1/2), (1/2, 1/2))
\end{equation}

\subsubsection{ESS}
Now let us find the ESS for the matching pennies game. In the same way we calculated the prisoner's dilemma ESS. We will assume the strategy  `Heads' is the population and the invader is the strategy `Tails'. Thus we have that the payoff for the strategy `Heads' is the following:
\begin{equation}
(1-{\epsilon})(1) + (-1){\epsilon} = 1 - 2{\epsilon}
\end{equation}
And the payoff for the strategy `Tails' is:
\begin{equation}
(1-{\epsilon})(-1) + (1){\epsilon} = 2{\epsilon} - 1
\end{equation}
We can see that both strategies appear to be equivalent unless there is a very small value of $\epsilon$.But since our simulation every time gives almost equal starting proportions to each strategy we will assume that the `invading' strategy `Tails' appears in a proportion of $\epsilon$=0.5. And we calculate as follows:
\begin{equation}
1- 2(0.5) = 0
\end{equation}
And for the strategy `Defect' is:
\begin{equation}
 2(0.5)- 1 = 0
\end{equation}
So we have that the expected payoff for both is 0 thus we can say that `Heads' is not an evolutionarily stable strategy, Given that it does not dominate `Tails'.
Now we check if the strategy `Tails' is evolutionarily stable. And therefore we assume that `Heads' is the invader. We will check the same way. We first have the expected payoff of the strategy `Tails':
\begin{equation}
(1-{\epsilon})(-1) + (1)({\epsilon}) = 2{\epsilon} -1
\end{equation}
And the payoff for the strategy `Heads' is:
\begin{equation}
(1-{\epsilon})(1) + (-1)({\epsilon}) =  1 - 2{\epsilon}
\end{equation}
Again it appears that they are equal. So we can see that `Tails' is not an ESS either. But we calculate it anyway assuming that the `invading' strategy `Heads' appears in a proportion of $\epsilon$=0.5. And we have the following for the strategy `Tails':
\begin{equation}
1 -  2(0.5) = 0
\end{equation}
And for the strategy `Heads' is:
\begin{equation}
 2(0.5) - 1 = 0
\end{equation}
So we see that again the strategy `Tails' is not an ESS either. And this explains the behaviour in our graph, how through time both strategies appear to interact and oscilate in the middle (0.5)



\subsection{``Battle of sexes''}

\subsubsection{Nash Equilibrium}
We can find the Nash equilibrium for the battle of sexes game, we can see by looking at the normal form table that there are 2 non symmetric Nash equilibria, this means that they do not play the same strategy; and there can also be found a mixed Nash equilibrium.

\begin{table}[H]
\begin{center}
Female

Male
\begin{tabular}{|l|c|c|}
\hline
 & Opera & Football\\ 
\hline
Opera & \underline{2}, \underline{1} & 0, 0\\
\hline
Football & 0, 0 & \underline{1}, \underline{2}\\
\hline
\end{tabular}

\caption{ Battle of sexes bimatrix payoff.}
\label{fig:mpnashbos}	
\end{center}
\end{table}

So we calculate the mixed Nash equilibrium by making equal the expected payoff  when going to the Opera to the expected payoff of watching Football. We will assume that the probability for the Male player of going to the Opera is $\sigma$ and for watching Football 1 - $\sigma$, resulting in the following:
\begin{equation}
(2)(\sigma) + (1 - \sigma)(0) = (0)(\sigma) + (1)(1 - \sigma)
\end{equation}
\begin{equation}
3\sigma = 1
\end{equation}
\begin{equation}
\sigma = 1/3
\end{equation}
This means that the Male player of chooses to go to the opera with the probability of 1/3 , and also means he will prefer watching football 2/3. And now we calculate the probability of going to the Opera for the Female player with probability $\sigma$ and for watching Football with probability 1 - $\sigma$.

\begin{equation}
(1)(\sigma) + (1 - \sigma)(0) = (0)(\sigma) + (2)(1 - \sigma)
\end{equation}
\begin{equation}
3\sigma = 2
\end{equation}
\begin{equation}
\sigma = 2/3
\end{equation}

This means that the Female player will choose to go to the Opera with a probability of 2/3 and in consequence only chooses to watch football the other 1/3. 
This means that the Nash equilibria is when:
\begin{equation}
((1/3, 2/3), (2/3, 1/3))
\end{equation}

\subsubsection{ESS}
Now let us find the ESS for this game. We will assume that going to the 'Opera' is the population and the invader is the strategy to watch `Football'. Thus we have that the payoff for the strategy `Opera' is the following:
\begin{equation}
(1-{\epsilon})(2) + (0){\epsilon} = 1 - 2{\epsilon}
\end{equation}
And the payoff for the strategy watching `Football' is:
\begin{equation}
(1-{\epsilon})(0) + (1){\epsilon} = {\epsilon}
\end{equation}
We can see that with a value of $\epsilon$ $\leq$ 0.3 `Opera' is an ESS. But since our simulation every time gives almost equal starting proportions to each strategy we will assume that the `invading' strategy `Football' appears in a proportion of $\epsilon$=0.5. And we calculate as follows:
\begin{equation}
1- 2(0.5) = 0
\end{equation}
And for the strategy `Football' is:
\begin{equation}
 (0.5) = 0.5
\end{equation}
So we have that the expected payoff for `Football' is greater thus we can say that `Opera' is not an evolutionarily stable strategy, Given that it does not dominate  `Football'.
Now we check if the strategy `Football' is evolutionarily stable. And therefore we assume that `Opera' is the invader. We will check the same way. We first have the expected payoff of the strategy `Football':
\begin{equation}
(1-{\epsilon})(1) + (0)({\epsilon}) = 1 - {\epsilon} 
\end{equation}
And the payoff for the strategy `Opera' is:
\begin{equation}
(1-{\epsilon})(0) + (2)({\epsilon}) =  2{\epsilon}
\end{equation}
So we can see that `Football' is not an ESS either, given that under certain values of $\epsilon$ it will not be dominant. Now we calculate assuming that the `invading' strategy `Opera' appears in a proportion of $\epsilon$=0.5. And we have the following for the strategy `Football':
\begin{equation}
1 -  (0.5) = 0.5
\end{equation}
And for the strategy `Opera' is:
\begin{equation}
 2(0.5)  = 1
\end{equation}
So we see that the strategy `Football' is not an ESS either. What we will have is that each strategy will appear to be evolutionarily stable according to their probability of appearance in the population. But with the default settings for our simulation which give a proportion of 0.5 to each there is not a clear evolutionarily stable strategy at the beginning of each simulation.


\subsection{``Hawk-dove''}


\subsubsection{Nash Equilibrium}
We will now find the Nash equilibrium for the hawk-dove game. The Nash equilibrium for the this game,  can be found by looking at the normal form table and select the best responses for each player. There are 2 Nash equilibria in this game.

\begin{table}[H]
\begin{center}
Player 2

Player 1
\begin{tabular}{|l|c|c|}
\hline
 & Hawk & Dove\\ 
\hline
Hawk & 0, 0 & \underline{3}, \underline{1}\\
\hline
Dove & \underline{1},\underline{3} & 2, 2\\
\hline
\end{tabular}

\caption{ Hawk-Dove bimatrix payoff.}
\label{fig:mpnashhd}	
\end{center}
\end{table}

So we calculate the mixed Nash equilibrium by making equal the expected payoff  when playing `Hawk' to the expected payoff of playing `Dove'. We will assume that the probability for player 1 of playing `Hawk' is $\sigma$ and for playing `Dove' is 1 - $\sigma$, resulting in the following:
\begin{equation}
(0)(\sigma) + (1 - \sigma)(3) = (1)(\sigma) + (2)(1 - \sigma)
\end{equation}
\begin{equation}
-2 \sigma = -1
\end{equation}
\begin{equation}
\sigma = 1/2
\end{equation}
This means that player 1 will play `Hawk' 1/2 of the times, and also means she will be playing `Dove' with a probability of 1/2. And now we calculate the probability of playing `Hawk' for player 2 with probability $\sigma$ and playing `Dove' with probability 1 - $\sigma$.

\begin{equation}
(0)(\sigma) + (1 - \sigma)(3) = (1)(\sigma) + (2)(1 - \sigma)
\end{equation}
\begin{equation}
4 \sigma = 2
\end{equation}
\begin{equation}
\sigma = 1/2
\end{equation}

This means that player 2 when playing against player 1 will play `Hawk' with a probability of  1/2  and in consequence play `Dove' with the remaining probability of 1/2. 
This means that the Nash equilibria is when:
\begin{equation}
((1/2, 1/2), (1/2, 1/2))
\end{equation}
Therefore it can be seen that there exist 2 non symmetric Nash equilibria and a mixed Nash equilibrium.

\subsubsection{ESS}
Now let us find the ESS for this game. We assume the strategy  `Hawk' is the population and the invader is the strategy `Doves'. Thus we have that the payoff for the strategy `Hawk' is the following:
\begin{equation}
(1-{\epsilon})(0) + (3){\epsilon} = 3{\epsilon}
\end{equation}
And the payoff for the strategy `Dove' is:
\begin{equation}
(1-{\epsilon})(1) + (2){\epsilon} = 1 + {\epsilon}
\end{equation}
It can be seen that with certain values of $\epsilon$ $>$ 5 Hawk is an ESS. Since our simulation every time gives almost equal starting proportions to each strategy we will assume that the `invading' strategy `Dove' appears in a proportion of $\epsilon$=0.5. And we calculate as follows:
\begin{equation}
3(0.5) = 1.5
\end{equation}
And for the strategy `Dove' is:
\begin{equation}
 1 + (0.5) = 1.5
\end{equation}
So we have that the expected payoff for both are equal thus we can say that `Hawk' is not an evolutionarily stable strategy with an initial distribution of 0.5, given that it does not dominate `Dove'.
Now we check if the strategy `Dove' is evolutionarily stable. And therefore we assume that `Hawk' is the invader. We will check the same way. We first have the expected payoff of the strategy `Dove':
\begin{equation}
(1-{\epsilon})(0) + (3)({\epsilon}) = 3{\epsilon} 
\end{equation}
And the payoff for the strategy `Hawk' is:
\begin{equation}
(1-{\epsilon})(1) + (2)({\epsilon}) =  1 + {\epsilon}
\end{equation}
We can see a similar situation as the previous. So we can assume that `Dove' is not an ESS either. But we calculate it anyway assuming that the `invading' strategy `Hawk' appears in a proportion of $\epsilon$=0.5. And we have the following for the strategy `Dove':
\begin{equation}
3(0.5) = 1.5
\end{equation}
And for the strategy `Hawk' is:
\begin{equation}
 1 + (0.5) = 1.5
\end{equation}
The strategy `Dove' is not an ESS either under this distribution.


\newpage
\appendix
\section{\\ Git Commands} \label{app:gitA}

\begin{itemize}
	\item $\textbf{git status:}$ Used to see the what the status on the working directory. It will display two sections, one with the staged changes that are ready to be committed and another section displaying the changes that have not been staged, along with a small legend on the left side of the name of the file, that indicates what changes have been done to that file (deleted, modified, new file, etc.)

	\item $\textbf{git add:}$ Used to stage the changes we have made in the directory for the local repository clone specific files. Before using this command all changes that have been made in the directory are considered unstaged. This means that any new file added or any already existing file that has been modified has to be staged with this command in order to include them in the next commit, any changes made that have not been staged with $\textbf{git add}$ will not be considered in the next commit.

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{imggitadd}

\caption{Example of one file staged with git add.}
\label{fig:imggitadd}
\end{center}
\end{figure}

	\item $\textbf{git commit:}$ Takes a snapshot of the status of the local cloned repository, of course the files that considers are the ones that $textbf{git add}$ was previously used on. Creates a new point in the history of the project. Once this command is run, it commonly opens a text editor where I introduced the changes that were made in the files that are being changed in this commit.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.4]{gitcommitmsg}

\caption{Example after using git commit asking for message for the commit.}
\label{fig:imggitcommit}
\end{center}
\end{figure}

	\item $\textbf{git diff:}$ Using this command directly displays the content that has been changed in the project since the last   commit and that are not staged for the next commit. If the command is run $\textbf{git diff - - cached}$ it will display the changes that have been staged. If we wish to see staged and unstaged changes together we use $\textbf{git diff HEAD}$ and if there is no interest in watching the specific changes but we want to display more information than just the one presented by $\textbf{git status}$ we add $\textbf{- - stat}$ after the different options we had with $\textbf{git diff [options] - - stat}$ and this displays a summary of the changes.

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{gitdiff}

\caption{ Example of general output with git diff and git diff  HEAD - - stat (red color represent the deleted lines and green the added) .}
\label{fig:imggitdiff}
\end{center}
\end{figure}

	\item $\textbf{git fetch [remote-name]:}$ This command pulls all the data from the remote project that is not contained the current copy we hold in our local repository clone. Basically used to update with all the changes other users (contributors) to the project have made, but it does not merge it with the work we have done because this has to be done manually. The name given to the remote repository we created a clone from is usually origin.
	\item $\textbf{git branch:}$  Without any arguments the command gives a list of the existing branches in the local repository.The command followed with the name of the branch $\textbf{git branch (branchname)}$ will create a branch with that given name out of the main project line (master), in the last commit made before the branch. So if we continue working on the master and then we switch to the branch all the changes made to that point will be reverted to the context where the branch was created.  If we wish to delete a local branch we use $\textbf{git branch -d (branchname)}$. To delete a remote branch we use the command $\textbf{git push (remote-name) :(branchname)}$.
	\item $\textbf{git checkout (branchname):}$ This command is used to change between the different existing branches. A way of creating a branch and changing to it at the same time is a shortcut provided by Git $\textbf{git checkout -b (branchname)}$. 
	\item $\textbf{git merge:}$ This command is used to merge the changes that have been made in a branch are complete. And what is usually done is change into the main project line (master) with $\textbf{git checkout (name of the branch we want to merge into)}$ and then use $\textbf{git merge (name of the branch wished to be merged in)}$. This way the changes, if there exists no conflict, will be merged into the main project line (master).
	\item $\textbf{git pull:}$ Basically does first a $\textbf{git fetch}$ immediately followed by a $\textbf{git merge}$ from the tracked branch into the branch we are currently in. This command basically does the same as those two commands, but it can if there are changes that may cause a problem, it might make the process a bit more complex. 

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.7]{gittreec}

\caption{ Example diagram from Github of a master branch with a branch 1 for being merged.}
\label{fig:imggitpull}
\end{center}
\end{figure}

	\item $\textbf{git push:}$ This command helps to update the local changes we have made to the remote repository. And the way to use it is $\textbf{git push (name given to the remote) (name of the local branch)}$. We just need to be careful to not overwrite changes. 
	\item $\textbf{git log:}$  This command displays all the commit messages that have been previously used in the state of the project we currently are in. There is an alternative command that displays how the project has been modified up to the point we currently are in and is $\textbf{git log - - oneline  - - decorate - - graph - - all}$

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.5]{gitlog1}

\caption{Basic example of git log oneline.}
\label{fig:imggitlog}
\end{center}
\end{figure}

\end{itemize}

\newpage
\section{\\Table of simulations for matching pennies game} \label{app:mptable}
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Num. & Population & Generations& Rounds per Gen. & Death Rate & Mutation Rate & Exploitation Rate\\ 
\hline
1& 1000 & 100 & 100 & 0.1 & 0.1 & 1\\
\hline
2& 1000 & 100 & 25 & 0.1 & 0.1 & 1\\
\hline
3& 1000 & 100 & 5 & 0.1 & 0.1 & 1\\
\hline
4& 1000 & 100 & 100, 25, 5 & 0.5 & 0.1 & 1\\
\hline
5& 1000 & 100 & 100, 25, 5 & 0.9 & 0.1 & 1\\
\hline
6& 1000 & 100 & 100, 25, 5 & 0.9 & 0.5 & 1\\
\hline
7& 1000 & 100 & 100, 25, 5 & 0.9 & 0.9 & 1\\
\hline
8& 1000 & 100 & 100, 25, 5 & 0.1 & 0.5 & 1\\
\hline
9& 1000 & 100 & 100, 25, 5 & 0.1 & 0.9 & 1\\
\hline
10& 1000 & 100 & 100, 25, 5 & 0.1 & 0.1 & 0.5\\
\hline
11& 1000 & 100 & 100, 25, 5 & 0.5 & 0.1 & 0.5\\
\hline
12& 1000 & 100 & 100, 25, 5 & 0.9 & 0.1 & 0.5\\
\hline
13& 1000 & 100 & 100, 25, 5 & 0.1 & 0.5 & 0.5\\
\hline
14& 1000 & 100 & 100, 25, 5 & 0.5 & 0.5 & 0.5\\
\hline
15& 1000 & 100 & 100, 25, 5 & 0.9 & 0.5 & 0.5\\
\hline
16& 1000 & 100 & 100, 25, 5 & 0.1 & 0.9 & 0.5\\
\hline
17& 1000 & 100 & 100, 25, 5 & 0.5 & 0.9 & 0.5\\
\hline
18& 1000 & 100 & 100, 25, 5 & 0.9 & 0.9 & 0.5\\
\hline
19& 1000 & 100 & 100, 25, 5 & 0.1 & 0.1 & 0.1\\
\hline
20& 1000 & 100 & 100, 25, 5 & 0.5 & 0.1 & 0.1\\
\hline
21& 1000 & 100 & 100, 25, 5 & 0.9 & 0.1 & 0.1\\
\hline
22& 1000 & 100 & 100, 25, 5 & 0.1 & 0.5 & 0.1\\
\hline
23& 1000 & 100 & 100, 25, 5 & 0.5 & 0.5 & 0.1\\
\hline
24& 1000 & 100 & 100, 25, 5 & 0.9 & 0.5 & 0.1\\
\hline
25& 1000 & 100 & 100, 25, 5 & 0.1 & 0.9& 0.1\\
\hline
26& 1000 & 100 & 100, 25, 5 & 0.5 & 0.9 & 0.1\\
\hline
27& 1000 & 100 & 100, 25, 5 & 0.9 & 0.9 & 0.1\\
\hline
28& 1000 & 100 & 100, 25, 5 & 0.5 & 0.5 & 1\\
\hline
29& 1000 & 100 & 100, 25, 5 & 0.5 & 0.9 & 1\\
\hline

\end{tabular}
\end{center}
\caption{Configuration of simulations for prisoner's dilemma.}
\label{tab:simcombtag}
\end{table}




\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}